% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{eia2024}
MikeAI70B and Miguel73487, ``Empathy-in-action: Measuring empathy in action,''
  \url{https://github.com/MikeAI70B/empathy-in-action}, 2024, behavioral
  empathy benchmark with 5 game-based scenarios. Paper preprint forthcoming.

\bibitem{marks2023geometry}
S.~Marks and M.~Tegmark, ``The geometry of truth: Emergent linear structure in
  large language model representations of true/false datasets,'' \emph{arXiv
  preprint arXiv:2310.06824}, 2023.

\bibitem{zou2023representation}
A.~Zou, L.~Phan, S.~Chen, J.~Campbell, P.~Guo, R.~Ren, A.~Pan, X.~Yin,
  M.~Mazeika, A.-K. Dombrowski \emph{et~al.}, ``Representation engineering: A
  top-down approach to ai transparency,'' \emph{arXiv preprint
  arXiv:2310.01405}, 2023.

\bibitem{elhage2022toy}
\BIBentryALTinterwordspacing
N.~Elhage, T.~Hume, C.~Olsson, N.~Schiefer, T.~Henighan, S.~Kravec,
  Z.~Hatfield-Dodds, R.~Lasenby, D.~Drain, C.~Chen \emph{et~al.}, ``Toy models
  of superposition,'' \emph{Transformer Circuits Thread}, 2022. [Online].
  Available: \url{https://transformer-circuits.pub/2022/toy_model/index.html}
\BIBentrySTDinterwordspacing

\bibitem{park2023linear}
K.~Park, Y.~J. Choe, and V.~Veitch, ``The linear representation hypothesis and
  the geometry of large language models,'' \emph{arXiv preprint
  arXiv:2311.03658}, 2023.

\bibitem{turner2023activation}
A.~Turner, L.~Thiergart, D.~Udell, N.~Nanda, T.~Rauker, and R.~Shah,
  ``Activation addition: Steering language models without optimization,''
  \emph{arXiv preprint arXiv:2308.10248}, 2023.

\bibitem{li2024inference}
K.~Li, O.~Patel, F.~Vieira, T.~Lukasiewicz, and A.~Weller, ``Inference-time
  intervention: Eliciting truthful answers from a language model,''
  \emph{Advances in Neural Information Processing Systems}, vol.~36, 2024.

\bibitem{jain2024mechanistically}
S.~Jain, R.~Kirk, E.~S. Lubana, A.~Geiger, S.~Serrano, S.~Marks, and N.~Nanda,
  ``Mechanistically analyzing the effects of fine-tuning on procedurally
  defined tasks,'' \emph{arXiv preprint arXiv:2311.12786}, 2024.

\bibitem{huang2023catastrophic}
Y.~Huang, S.~Gupta, M.~Xia, K.~Li, and D.~Chen, ``Catastrophic jailbreak of
  open-source llms via exploiting generation,'' \emph{arXiv preprint
  arXiv:2310.06987}, 2023.

\bibitem{abdin2024phi3}
M.~Abdin, S.~A. Jacobs, A.~A. Awan, J.~Aneja, A.~Awadallah, H.~Awadalla,
  N.~Bach, A.~Bahree, A.~Bakhtiari, H.~Behl \emph{et~al.}, ``Phi-3 technical
  report: A highly capable language model locally on your phone,'' \emph{arXiv
  preprint arXiv:2404.14219}, 2024.

\end{thebibliography}
