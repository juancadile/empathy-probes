{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Model Probe Agreement: Qwen & Dolphin\n",
    "\n",
    "This notebook tests whether Qwen and Dolphin probes agree with Phi-3 probe on **the same text**.\n",
    "\n",
    "**What it does:**\n",
    "1. Downloads Phi-3 probe directions for Qwen/Dolphin from GitHub\n",
    "2. Downloads the actual 12 completions that Phi-3 generated for EIA testing\n",
    "3. Runs those same completions through Qwen/Dolphin probes\n",
    "4. Computes correlation with the ground-truth empathy scores\n",
    "\n",
    "**Key insight:**\n",
    "- Phi-3 probe achieved r=0.71 on these completions\n",
    "- If Qwen/Dolphin probes also correlate well, it shows cross-model agreement\n",
    "- If they don't correlate, the probes measure different things\n",
    "\n",
    "**This is NOT testing:**\n",
    "- How well each model generates empathic completions (different test)\n",
    "- Behavioral empathy of Qwen/Dolphin themselves (would need separate EIA)\n",
    "\n",
    "**This IS testing:**\n",
    "- Do probes from different architectures agree on what counts as empathic text?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch transformers accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pickle\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected!\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cases (Same as Phi-3)\n",
    "\n",
    "These are the 12 test cases with human-scored empathy levels:\n",
    "- Score 0 = non-empathic (task-focused)\n",
    "- Score 1 = moderate empathy (balanced)\n",
    "- Score 2 = highly empathic (person-focused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Phi-3 EIA correlation results from GitHub\n",
    "# We'll use the SAME completions that Phi-3 generated as our test set\n",
    "# This tests: \"Do Qwen/Dolphin probes agree with Phi-3 probe on the same text?\"\n",
    "\n",
    "import requests\n",
    "\n",
    "GITHUB_EIA = \"https://raw.githubusercontent.com/juancadile/empathy-probes/main/results/eia_correlation.json\"\n",
    "\n",
    "print(\"Downloading Phi-3 EIA test completions from GitHub...\")\n",
    "response = requests.get(GITHUB_EIA)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    phi3_eia_data = response.json()\n",
    "    \n",
    "    # These are the actual completions Phi-3 generated\n",
    "    # We'll test if Qwen/Dolphin probes give similar scores\n",
    "    TEST_CASES = []\n",
    "    \n",
    "    for result in phi3_eia_data['detailed_results']:\n",
    "        TEST_CASES.append({\n",
    "            'scenario': result['scenario'],\n",
    "            'true_score': result['true_score'],\n",
    "            'expected_category': result['expected_category'],\n",
    "            'text': result['text_preview']  # Using preview for now\n",
    "        })\n",
    "    \n",
    "    print(f\"\u2713 Loaded {len(TEST_CASES)} test cases from Phi-3\")\n",
    "    print(f\"  High empathy (2): {sum(1 for t in TEST_CASES if t['true_score'] == 2)}\")\n",
    "    print(f\"  Medium empathy (1): {sum(1 for t in TEST_CASES if t['true_score'] == 1)}\")\n",
    "    print(f\"  Low empathy (0): {sum(1 for t in TEST_CASES if t['true_score'] == 0)}\")\n",
    "    print(f\"\\nPhi-3 probe achieved: r={phi3_eia_data['pearson_correlation']:.3f}\")\n",
    "    print(\"Testing if Qwen/Dolphin probes agree on the same text...\\n\")\n",
    "else:\n",
    "    print(f\"\u2717 Failed to download (HTTP {response.status_code})\")\n",
    "    TEST_CASES = []\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Probes from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_BASE = \"https://raw.githubusercontent.com/juancadile/empathy-probes/main/results/cross_model_validation\"\n",
    "\n",
    "# Qwen uses layer 16, Dolphin uses layer 8 (optimal layers from validation)\n",
    "MODELS_CONFIG = {\n",
    "    \"qwen2.5-7b\": {\n",
    "        \"model_name\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        \"layer\": 16,\n",
    "        \"probe_file\": \"qwen2.5-7b_layer16_probe.npy\"\n",
    "    },\n",
    "    \"dolphin-llama-3.1-8b\": {\n",
    "        \"model_name\": \"cognitivecomputations/dolphin-2.9.4-llama3.1-8b\",\n",
    "        \"layer\": 8,\n",
    "        \"probe_file\": \"dolphin-llama-3.1-8b_layer8_probe.npy\"\n",
    "    }\n",
    "}\n",
    "\n",
    "probes = {}\n",
    "\n",
    "for model_key, config in MODELS_CONFIG.items():\n",
    "    print(f\"\\nDownloading probe for {model_key} (layer {config['layer']})...\")\n",
    "    url = f\"{GITHUB_BASE}/{config['probe_file']}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            filename = config['probe_file']\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            # Load probe (.npy file)\n",
    "            probe_direction = np.load(filename)\n",
    "            \n",
    "            probes[model_key] = {\n",
    "                'direction': torch.tensor(probe_direction, dtype=torch.float16).to(device),\n",
    "                'layer': config['layer'],\n",
    "                'source': 'cross_model_validation'\n",
    "            }\n",
    "            \n",
    "            print(f\"  \u2713 Loaded probe ({probe_direction.shape})\")\n",
    "        else:\n",
    "            print(f\"  \u2717 Failed (HTTP {response.status_code})\")\n",
    "            print(f\"  Tried: {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  \u2717 Error: {e}\")\n",
    "\n",
    "print(f\"\\n\u2705 Loaded probes for {len(probes)} models\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Completions & Compute Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_activations(model, tokenizer, text: str, layer: int) -> torch.Tensor:\n",
    "    \"\"\"Extract activations from specified layer.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[layer]  # (batch, seq_len, hidden_dim)\n",
    "        \n",
    "        # Use last token activation\n",
    "        last_token_activation = hidden_states[0, -1, :]\n",
    "        \n",
    "    return last_token_activation\n",
    "\n",
    "\n",
    "def compute_probe_projection(activation: torch.Tensor, probe_direction: torch.Tensor) -> float:\n",
    "    \"\"\"Compute projection onto probe direction.\"\"\"\n",
    "    # Ensure same dtype\n",
    "    activation = activation.to(torch.float32)\n",
    "    probe_direction = probe_direction.to(torch.float32)\n",
    "    \n",
    "    projection = torch.dot(activation, probe_direction).item()\n",
    "    return projection\n",
    "\n",
    "\n",
    "def analyze_model(model_key: str, config: dict, probe_info: dict, test_cases: List[dict]) -> dict:\n",
    "    \"\"\"Run full behavioral correlation analysis for one model.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing: {model_key}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"Loading {config['model_name']}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config['model_name'], trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config['model_name'],\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"\u2713 Model loaded (Layer {probe_info['layer']})\\n\")\n",
    "    \n",
    "    # Generate completions and compute projections\n",
    "        # Use the pre-generated text from test case (not generating new)\n",
    "        response = test_case[\"text\"]\n",
    "        generated_text = response  # Already have the completion\n",
    "        \n",
    "        # Generate completion\n",
    "        # Use the pre-generated text from test case (not generating new)\n",
    "        response = test_case[\"text\"]\n",
    "        generated_text = response  # Already have the completion\n",
    "        \n",
    "        \n",
    "        # Print first 3 outputs to inspect quality\n",
    "        if i <= 3:\n",
    "            print(f\"\\n  Generated: {response[:200]}...\")\n",
    "        \n",
    "        # Extract activation and compute probe projection\n",
    "        activation = extract_activations(model, tokenizer, generated_text, probe_info['layer'])\n",
    "        probe_score = compute_probe_projection(activation, probe_info['direction'])\n",
    "        \n",
    "        probe_scores.append(probe_score)\n",
    "        true_scores.append(test_case['true_score'])\n",
    "        \n",
    "        results.append({\n",
    "            \"scenario\": test_case['scenario'],\n",
    "            \"true_score\": test_case['true_score'],\n",
    "            \"probe_score\": probe_score,\n",
    "            \"expected_category\": test_case['category'],\n",
    "            \"text_preview\": response[:150]\n",
    "        })\n",
    "        \n",
    "        print(f\"\u2713 (score: {probe_score:.2f})\")\n",
    "    \n",
    "    # Compute correlations\n",
    "    pearson_r, pearson_p = pearsonr(probe_scores, true_scores)\n",
    "    spearman_r, spearman_p = spearmanr(probe_scores, true_scores)\n",
    "    \n",
    "    # Binary classification (empathic vs non-empathic)\n",
    "    # Use median probe score as threshold\n",
    "    median_score = np.median(probe_scores)\n",
    "    binary_predictions = [1 if score > median_score else 0 for score in probe_scores]\n",
    "    binary_true = [1 if score >= 1 else 0 for score in true_scores]\n",
    "    \n",
    "    # Confusion matrix\n",
    "    tp = sum(1 for p, t in zip(binary_predictions, binary_true) if p == 1 and t == 1)\n",
    "    fp = sum(1 for p, t in zip(binary_predictions, binary_true) if p == 1 and t == 0)\n",
    "    tn = sum(1 for p, t in zip(binary_predictions, binary_true) if p == 0 and t == 0)\n",
    "    fn = sum(1 for p, t in zip(binary_predictions, binary_true) if p == 0 and t == 1)\n",
    "    \n",
    "    binary_accuracy = (tp + tn) / len(binary_true) if len(binary_true) > 0 else 0\n",
    "    \n",
    "    # Clean up model to free memory\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        \"model\": model_key,\n",
    "        \"model_name\": config['model_name'],\n",
    "        \"layer_used\": probe_info['layer'],\n",
    "        \"pearson_correlation\": float(pearson_r),\n",
    "        \"pearson_p_value\": float(pearson_p),\n",
    "        \"spearman_correlation\": float(spearman_r),\n",
    "        \"spearman_p_value\": float(spearman_p),\n",
    "        \"binary_accuracy\": float(binary_accuracy),\n",
    "        \"confusion_matrix\": [[tp, fp], [fn, tn]],\n",
    "        \"num_test_cases\": len(test_cases),\n",
    "        \"detailed_results\": results\n",
    "    }\n",
    "\n",
    "print(\"Functions defined. Ready to run analysis.\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Analysis for Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "\n",
    "for model_key in MODELS_CONFIG.keys():\n",
    "    if model_key in probes:\n",
    "        result = analyze_model(\n",
    "            model_key=model_key,\n",
    "            config=MODELS_CONFIG[model_key],\n",
    "            probe_info=probes[model_key],\n",
    "            test_cases=TEST_CASES\n",
    "        )\n",
    "        all_results[model_key] = result\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"RESULTS for {model_key}:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Pearson r: {result['pearson_correlation']:.3f} (p={result['pearson_p_value']:.4f})\")\n",
    "        print(f\"Spearman \u03c1: {result['spearman_correlation']:.3f} (p={result['spearman_p_value']:.4f})\")\n",
    "        print(f\"Binary accuracy: {result['binary_accuracy']*100:.1f}%\")\n",
    "        print(f\"Confusion matrix: {result['confusion_matrix']}\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"\\n\u26a0\ufe0f  Skipping {model_key} (probe not loaded)\")\n",
    "\n",
    "print(\"\\n\u2705 All analyses complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save individual results\n",
    "for model_key, result in all_results.items():\n",
    "    filename = f\"{model_key}_eia_correlation.json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "# Save combined results\n",
    "combined_filename = \"all_models_eia_correlation.json\"\n",
    "with open(combined_filename, 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "print(f\"Saved: {combined_filename}\")\n",
    "\n",
    "# Download files\n",
    "from google.colab import files\n",
    "\n",
    "for model_key in all_results.keys():\n",
    "    filename = f\"{model_key}_eia_correlation.json\"\n",
    "    files.download(filename)\n",
    "\n",
    "files.download(combined_filename)\n",
    "\n",
    "print(\"\\n\u2705 All files downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "After running this notebook, you'll have:\n",
    "- `qwen2.5-7b_eia_correlation.json` - Qwen correlation results\n",
    "- `dolphin-llama-3.1-8b_eia_correlation.json` - Dolphin correlation results\n",
    "- `all_models_eia_correlation.json` - Combined results\n",
    "\n",
    "**Next steps:**\n",
    "1. Upload these files to `/results/` in the repo\n",
    "2. Update paper Table 2 to include all 3 models\n",
    "3. Update behavioral correlation text to report all 3 correlations"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}