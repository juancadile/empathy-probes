{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empathy Probe Results Analysis\n",
    "\n",
    "Interactive notebook for analyzing empathy probe extraction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Paths\n",
    "RESULTS_DIR = Path('../results')\n",
    "PROBES_DIR = RESULTS_DIR / 'probes'\n",
    "FIGURES_DIR = RESULTS_DIR / 'figures'\n",
    "FIGURES_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation results\n",
    "with open(RESULTS_DIR / 'validation_auroc.json') as f:\n",
    "    validation_results = json.load(f)\n",
    "\n",
    "# Load EIA correlation results\n",
    "with open(RESULTS_DIR / 'eia_correlation.json') as f:\n",
    "    eia_results = json.load(f)\n",
    "\n",
    "# Load steering results (if available)\n",
    "steering_path = RESULTS_DIR / 'steering_examples.json'\n",
    "if steering_path.exists():\n",
    "    with open(steering_path) as f:\n",
    "        steering_results = json.load(f)\n",
    "else:\n",
    "    steering_results = None\n",
    "\n",
    "print(\"‚úì Results loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Layer-wise Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract layer-wise metrics\n",
    "layers = []\n",
    "aurocs = []\n",
    "accuracies = []\n",
    "separations = []\n",
    "\n",
    "for layer, results in validation_results['layer_results'].items():\n",
    "    layers.append(int(layer))\n",
    "    aurocs.append(results['auroc'])\n",
    "    accuracies.append(results['accuracy'])\n",
    "    separations.append(results['separation'])\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# AUROC by layer\n",
    "axes[0].plot(layers, aurocs, 'o-', linewidth=2, markersize=8, color='steelblue')\n",
    "axes[0].axhline(y=0.75, color='red', linestyle='--', label='Target (0.75)')\n",
    "axes[0].set_xlabel('Layer', fontsize=12)\n",
    "axes[0].set_ylabel('AUROC', fontsize=12)\n",
    "axes[0].set_title('AUROC by Layer', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy by layer\n",
    "axes[1].plot(layers, accuracies, 'o-', linewidth=2, markersize=8, color='green')\n",
    "axes[1].axhline(y=0.75, color='red', linestyle='--', label='Target (0.75)')\n",
    "axes[1].set_xlabel('Layer', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Accuracy by Layer', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Separation by layer\n",
    "axes[2].plot(layers, separations, 'o-', linewidth=2, markersize=8, color='purple')\n",
    "axes[2].set_xlabel('Layer', fontsize=12)\n",
    "axes[2].set_ylabel('Mean Separation', fontsize=12)\n",
    "axes[2].set_title('Empathic vs Non-Empathic Separation', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'layer_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best layer: {validation_results['best_layer']}\")\n",
    "print(f\"Best AUROC: {validation_results['best_auroc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EIA Score Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract detailed results\n",
    "detailed = eia_results['detailed_results']\n",
    "\n",
    "# Create scatter plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "scenarios = [d['scenario'] for d in detailed]\n",
    "true_scores = [d['true_score'] for d in detailed]\n",
    "probe_scores = [d['probe_score'] for d in detailed]\n",
    "\n",
    "# Color by scenario\n",
    "scenario_colors = {\n",
    "    'food_delivery': 'red',\n",
    "    'the_listener': 'blue',\n",
    "    'the_maze': 'green',\n",
    "    'the_protector': 'purple',\n",
    "    'the_duel': 'orange'\n",
    "}\n",
    "\n",
    "for scenario in set(scenarios):\n",
    "    mask = [s == scenario for s in scenarios]\n",
    "    true_filtered = [t for t, m in zip(true_scores, mask) if m]\n",
    "    probe_filtered = [p for p, m in zip(probe_scores, mask) if m]\n",
    "    ax.scatter(true_filtered, probe_filtered, \n",
    "               label=scenario, color=scenario_colors.get(scenario, 'gray'),\n",
    "               s=100, alpha=0.7)\n",
    "\n",
    "# Add correlation line\n",
    "from scipy.stats import linregress\n",
    "slope, intercept, r_value, p_value, std_err = linregress(true_scores, probe_scores)\n",
    "x_line = np.array([0, 2])\n",
    "y_line = slope * x_line + intercept\n",
    "ax.plot(x_line, y_line, 'k--', alpha=0.5, label=f'Linear fit (r={r_value:.3f})')\n",
    "\n",
    "ax.set_xlabel('True EIA Score', fontsize=12)\n",
    "ax.set_ylabel('Probe Projection Score', fontsize=12)\n",
    "ax.set_title(f'EIA Score vs Probe Projection (r={eia_results[\"pearson_correlation\"]:.3f})', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'eia_correlation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Pearson correlation: {eia_results['pearson_correlation']:.4f}\")\n",
    "print(f\"Binary accuracy: {eia_results['binary_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Steering Examples Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if steering_results is not None:\n",
    "    for i, exp in enumerate(steering_results['experiments']):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"SCENARIO: {exp['scenario'].upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"\\nExpected change: {exp['expected_change']}\")\n",
    "        print(f\"\\n--- BASELINE (Œ±=0) ---\")\n",
    "        print(exp['baseline'][:300] + \"...\")\n",
    "        \n",
    "        for steered in exp['steered_completions']:\n",
    "            print(f\"\\n--- STEERED (Œ±={steered['alpha']}) ---\")\n",
    "            print(steered['completion'][:300] + \"...\")\n",
    "else:\n",
    "    print(\"Steering results not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Probe Vector Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load probe vectors\n",
    "best_layer = validation_results['best_layer']\n",
    "probe_path = PROBES_DIR / f'empathy_direction_layer_{best_layer}.npy'\n",
    "empathy_direction = np.load(probe_path)\n",
    "\n",
    "print(f\"Empathy direction shape: {empathy_direction.shape}\")\n",
    "print(f\"Norm: {np.linalg.norm(empathy_direction):.4f}\")\n",
    "print(f\"Mean: {empathy_direction.mean():.6f}\")\n",
    "print(f\"Std: {empathy_direction.std():.6f}\")\n",
    "\n",
    "# Visualize distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(empathy_direction, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_xlabel('Value', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title(f'Empathy Direction Distribution (Layer {best_layer})', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Top magnitudes\n",
    "top_k = 20\n",
    "top_indices = np.argsort(np.abs(empathy_direction))[-top_k:]\n",
    "top_values = empathy_direction[top_indices]\n",
    "\n",
    "colors = ['red' if v < 0 else 'green' for v in top_values]\n",
    "axes[1].barh(range(top_k), top_values, color=colors, alpha=0.7)\n",
    "axes[1].set_xlabel('Coefficient Value', fontsize=12)\n",
    "axes[1].set_ylabel('Dimension Index', fontsize=12)\n",
    "axes[1].set_title(f'Top {top_k} Dimensions by Magnitude', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'probe_vector_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EMPATHY PROBE EXTRACTION - SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä VALIDATION RESULTS:\")\n",
    "print(f\"  Best Layer: {validation_results['best_layer']}\")\n",
    "print(f\"  Best AUROC: {validation_results['best_auroc']:.4f}\")\n",
    "print(f\"  Target achieved (>0.75): {'‚úì YES' if validation_results['best_auroc'] >= 0.75 else '‚úó NO'}\")\n",
    "\n",
    "print(\"\\nüéØ EIA SCORE PREDICTION:\")\n",
    "print(f\"  Pearson correlation: {eia_results['pearson_correlation']:.4f}\")\n",
    "print(f\"  Spearman correlation: {eia_results['spearman_correlation']:.4f}\")\n",
    "print(f\"  Binary accuracy (0 vs 2): {eia_results['binary_accuracy']:.4f}\")\n",
    "print(f\"  Target achieved (r>0.4): {'‚úì YES' if abs(eia_results['pearson_correlation']) >= 0.4 else '‚úó NO'}\")\n",
    "\n",
    "if steering_results is not None:\n",
    "    print(\"\\nüéõÔ∏è STEERING EXPERIMENTS:\")\n",
    "    print(f\"  Scenarios tested: {len(steering_results['experiments'])}\")\n",
    "    print(f\"  Alpha values tested: {steering_results['alphas_tested']}\")\n",
    "    print(\"  See detailed results above\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All visualizations saved to: results/figures/\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
