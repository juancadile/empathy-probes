{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUfwhZ_HggYw",
        "outputId": "59067a3f-b51c-4857-e704-54144c7788f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Nov 16 21:12:08 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#Check hardware status\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Initial Git Clone\n",
        "!git clone https://github.com/juancadile/empathy-probes.git\n",
        "%cd empathy-probes\n",
        "!git checkout cloud-strengthening\n",
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfMXGUmtgmOE",
        "outputId": "9b8016b4-30f5-4281-de2f-13d222760b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'empathy-probes'...\n",
            "remote: Enumerating objects: 319, done.\u001b[K\n",
            "remote: Counting objects: 100% (319/319), done.\u001b[K\n",
            "remote: Compressing objects: 100% (210/210), done.\u001b[K\n",
            "remote: Total 319 (delta 152), reused 271 (delta 107), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (319/319), 4.70 MiB | 37.27 MiB/s, done.\n",
            "Resolving deltas: 100% (152/152), done.\n",
            "/content/empathy-probes\n",
            "Branch 'cloud-strengthening' set up to track remote branch 'cloud-strengthening' from 'origin'.\n",
            "Switched to a new branch 'cloud-strengthening'\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Re-download (updated) content from GitHub\n",
        "%cd /content/empathy-probes\n",
        "!git pull origin cloud-strengthening"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESRbqDUtihXo",
        "outputId": "fed313e8-57d6-4472-f3fc-bb9ffaf9c48b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/empathy-probes\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (4/4), 374 bytes | 374.00 KiB/s, done.\n",
            "From https://github.com/juancadile/empathy-probes\n",
            " * branch            cloud-strengthening -> FETCH_HEAD\n",
            "   2d0b6ce..849ba12  cloud-strengthening -> origin/cloud-strengthening\n",
            "Updating 2d0b6ce..849ba12\n",
            "Fast-forward\n",
            " src/probe_extraction_cross_model.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 1 insertion(+), 1 deletion(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install reqs.\n",
        "!pip install -q transformers accelerate bitsandbytes datasets scikit-learn\n",
        "print(\"✓ Ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGEAa53RgoGk",
        "outputId": "973ac2d1-8ebd-4207-d981-786df0d6277d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✓ Ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Verify dataset\n",
        "!ls -la data/contrastive_pairs/\n",
        "!wc -l data/contrastive_pairs/*.jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFEgcdLGgqIC",
        "outputId": "c8d9554a-9b18-4a7d-938c-4e8ec39871a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 300\n",
            "drwxr-xr-x 2 root root   4096 Nov 16 21:12 .\n",
            "drwxr-xr-x 4 root root   4096 Nov 16 21:12 ..\n",
            "-rw-r--r-- 1 root root    299 Nov 16 21:12 dataset_summary.json\n",
            "-rw-r--r-- 1 root root  81778 Nov 16 21:12 test_pairs_ablated.jsonl\n",
            "-rw-r--r-- 1 root root  63840 Nov 16 21:12 test_pairs.jsonl\n",
            "-rw-r--r-- 1 root root 144841 Nov 16 21:12 train_pairs.jsonl\n",
            "    15 data/contrastive_pairs/test_pairs_ablated.jsonl\n",
            "    15 data/contrastive_pairs/test_pairs.jsonl\n",
            "    35 data/contrastive_pairs/train_pairs.jsonl\n",
            "    65 total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/probe_extraction_cross_model.py --models qwen2.5-7b dolphin-llama-3.1-8b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMLcwWb9guLR",
        "outputId": "5aa77d64-2f4d-41e9-e055-8ab237172785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-16 21:20:32,954 - INFO - Running probe extraction for: ['qwen2.5-7b', 'dolphin-llama-3.1-8b']\n",
            "2025-11-16 21:20:32,955 - INFO - Device: cuda\n",
            "\n",
            "2025-11-16 21:20:32,955 - INFO - \n",
            "================================================================================\n",
            "2025-11-16 21:20:32,955 - INFO - Processing qwen2.5-7b: Qwen/Qwen2.5-7B-Instruct\n",
            "2025-11-16 21:20:32,955 - INFO - ================================================================================\n",
            "\n",
            "2025-11-16 21:20:32,955 - INFO - Loading Qwen/Qwen2.5-7B-Instruct...\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "2025-11-16 21:20:34.369699: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763328034.388523    2980 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763328034.393701    2980 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763328034.407010    2980 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763328034.407032    2980 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763328034.407036    2980 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763328034.407042    2980 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 21:20:34.410980: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-16 21:20:41,536 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
            "Loading checkpoint shards: 100% 4/4 [01:06<00:00, 16.60s/it]\n",
            "2025-11-16 21:21:48,284 - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "2025-11-16 21:21:48,285 - INFO - ✓ Loaded Qwen/Qwen2.5-7B-Instruct on cuda\n",
            "2025-11-16 21:21:48,285 - INFO - Loading train and test datasets...\n",
            "2025-11-16 21:21:48,291 - INFO - Train: 35 pairs, Test: 15 pairs\n",
            "2025-11-16 21:21:48,291 - INFO - \n",
            "--- Layer 8 ---\n",
            "2025-11-16 21:21:48,291 - INFO - Extracting training activations...\n",
            "Layer 8: 100% 35/35 [00:32<00:00,  1.09it/s]\n",
            "Layer 8: 100% 35/35 [00:30<00:00,  1.14it/s]\n",
            "2025-11-16 21:22:51,256 - INFO - Computing probe direction...\n",
            "2025-11-16 21:22:51,261 - INFO - Extracting test activations...\n",
            "Layer 8: 100% 15/15 [00:13<00:00,  1.11it/s]\n",
            "Layer 8: 100% 15/15 [00:13<00:00,  1.08it/s]\n",
            "2025-11-16 21:23:18,637 - INFO - Validating probe...\n",
            "2025-11-16 21:23:18,669 - INFO - AUROC: 0.791, Accuracy: 66.7%\n",
            "2025-11-16 21:23:18,670 - INFO - Saved probe to /content/empathy-probes/results/cross_model_validation/qwen2.5-7b_layer8_probe.npy\n",
            "2025-11-16 21:23:18,670 - INFO - \n",
            "--- Layer 12 ---\n",
            "2025-11-16 21:23:18,670 - INFO - Extracting training activations...\n",
            "Layer 12: 100% 35/35 [00:32<00:00,  1.08it/s]\n",
            "Layer 12: 100% 35/35 [00:31<00:00,  1.12it/s]\n",
            "2025-11-16 21:24:22,361 - INFO - Computing probe direction...\n",
            "2025-11-16 21:24:22,362 - INFO - Extracting test activations...\n",
            "Layer 12: 100% 15/15 [00:13<00:00,  1.10it/s]\n",
            "Layer 12: 100% 15/15 [00:13<00:00,  1.08it/s]\n",
            "2025-11-16 21:24:49,916 - INFO - Validating probe...\n",
            "2025-11-16 21:24:49,919 - INFO - AUROC: 0.964, Accuracy: 86.7%\n",
            "2025-11-16 21:24:49,920 - INFO - Saved probe to /content/empathy-probes/results/cross_model_validation/qwen2.5-7b_layer12_probe.npy\n",
            "2025-11-16 21:24:49,920 - INFO - \n",
            "--- Layer 16 ---\n",
            "2025-11-16 21:24:49,920 - INFO - Extracting training activations...\n",
            "Layer 16: 100% 35/35 [00:32<00:00,  1.07it/s]\n",
            "Layer 16: 100% 35/35 [00:31<00:00,  1.12it/s]\n",
            "2025-11-16 21:25:53,727 - INFO - Computing probe direction...\n",
            "2025-11-16 21:25:53,728 - INFO - Extracting test activations...\n",
            "Layer 16: 100% 15/15 [00:13<00:00,  1.09it/s]\n",
            "Layer 16: 100% 15/15 [00:14<00:00,  1.06it/s]\n",
            "2025-11-16 21:26:21,621 - INFO - Validating probe...\n",
            "2025-11-16 21:26:21,626 - INFO - AUROC: 1.000, Accuracy: 93.3%\n",
            "2025-11-16 21:26:21,626 - INFO - Saved probe to /content/empathy-probes/results/cross_model_validation/qwen2.5-7b_layer16_probe.npy\n",
            "2025-11-16 21:26:21,626 - INFO - \n",
            "--- Layer 20 ---\n",
            "2025-11-16 21:26:21,626 - INFO - Extracting training activations...\n",
            "Layer 20: 100% 35/35 [00:32<00:00,  1.08it/s]\n",
            "Layer 20: 100% 35/35 [00:31<00:00,  1.12it/s]\n",
            "2025-11-16 21:27:25,321 - INFO - Computing probe direction...\n",
            "2025-11-16 21:27:25,322 - INFO - Extracting test activations...\n",
            "Layer 20: 100% 15/15 [00:13<00:00,  1.09it/s]\n",
            "Layer 20: 100% 15/15 [00:13<00:00,  1.07it/s]\n",
            "2025-11-16 21:27:53,011 - INFO - Validating probe...\n",
            "2025-11-16 21:27:53,014 - INFO - AUROC: 0.991, Accuracy: 96.7%\n",
            "2025-11-16 21:27:53,014 - INFO - Saved probe to /content/empathy-probes/results/cross_model_validation/qwen2.5-7b_layer20_probe.npy\n",
            "2025-11-16 21:27:53,015 - INFO - \n",
            "--- Layer 24 ---\n",
            "2025-11-16 21:27:53,015 - INFO - Extracting training activations...\n",
            "Layer 24: 100% 35/35 [00:32<00:00,  1.08it/s]\n",
            "Layer 24: 100% 35/35 [00:31<00:00,  1.12it/s]\n",
            "2025-11-16 21:28:56,772 - INFO - Computing probe direction...\n",
            "2025-11-16 21:28:56,773 - INFO - Extracting test activations...\n",
            "Layer 24: 100% 15/15 [00:13<00:00,  1.09it/s]\n",
            "Layer 24: 100% 15/15 [00:14<00:00,  1.07it/s]\n",
            "2025-11-16 21:29:24,542 - INFO - Validating probe...\n",
            "2025-11-16 21:29:24,545 - INFO - AUROC: 0.964, Accuracy: 93.3%\n",
            "2025-11-16 21:29:24,545 - INFO - Saved probe to /content/empathy-probes/results/cross_model_validation/qwen2.5-7b_layer24_probe.npy\n",
            "2025-11-16 21:29:24,546 - INFO - \n",
            "✓ Completed qwen2.5-7b\n",
            "2025-11-16 21:29:24,546 - INFO - Results saved to /content/empathy-probes/results/cross_model_validation/qwen2.5-7b_results.json\n",
            "\n",
            "2025-11-16 21:29:24,633 - INFO - \n",
            "================================================================================\n",
            "2025-11-16 21:29:24,634 - INFO - Processing dolphin-llama-3.1-8b: cognitivecomputations/dolphin-2.9.3-llama-3.1-8b\n",
            "2025-11-16 21:29:24,635 - INFO - ================================================================================\n",
            "\n",
            "2025-11-16 21:29:24,635 - INFO - Loading cognitivecomputations/dolphin-2.9.3-llama-3.1-8b...\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\", line 402, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/requests/models.py\", line 1026, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/cognitivecomputations/dolphin-2.9.3-llama-3.1-8b/resolve/main/tokenizer_config.json\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 479, in cached_files\n",
            "    hf_hub_download(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1007, in hf_hub_download\n",
            "    return _hf_hub_download_to_cache_dir(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1114, in _hf_hub_download_to_cache_dir\n",
            "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1655, in _raise_on_head_call_error\n",
            "    raise head_call_error\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1543, in _get_metadata_or_catch_error\n",
            "    metadata = get_hf_file_metadata(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1460, in get_hf_file_metadata\n",
            "    r = _request_wrapper(\n",
            "        ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 283, in _request_wrapper\n",
            "    response = _request_wrapper(\n",
            "               ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 307, in _request_wrapper\n",
            "    hf_raise_for_status(response)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\", line 452, in hf_raise_for_status\n",
            "    raise _format(RepositoryNotFoundError, message, response) from e\n",
            "huggingface_hub.errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-691a4234-236de4f168bdb7c270a45704;25179383-8511-492d-b32e-4174e1a28777)\n",
            "\n",
            "Repository Not Found for url: https://huggingface.co/cognitivecomputations/dolphin-2.9.3-llama-3.1-8b/resolve/main/tokenizer_config.json.\n",
            "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
            "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
            "Invalid username or password.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/empathy-probes/src/probe_extraction_cross_model.py\", line 326, in <module>\n",
            "    main()\n",
            "  File \"/content/empathy-probes/src/probe_extraction_cross_model.py\", line 300, in main\n",
            "    results = run_probe_extraction(model_key, args.device)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/empathy-probes/src/probe_extraction_cross_model.py\", line 203, in run_probe_extraction\n",
            "    model, tokenizer = load_model_and_tokenizer(model_name, device)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/empathy-probes/src/probe_extraction_cross_model.py\", line 49, in load_model_and_tokenizer\n",
            "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\", line 1073, in from_pretrained\n",
            "    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\", line 905, in get_tokenizer_config\n",
            "    resolved_config_file = cached_file(\n",
            "                           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 322, in cached_file\n",
            "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 511, in cached_files\n",
            "    raise OSError(\n",
            "OSError: cognitivecomputations/dolphin-2.9.3-llama-3.1-8b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/probe_extraction_cross_model.py --models dolphin-llama-3.1-8b\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pbKh4MRlPjq",
        "outputId": "09655866-ff5c-4918-e630-cba87313cba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-16 21:32:29,826 - INFO - Running probe extraction for: ['dolphin-llama-3.1-8b']\n",
            "2025-11-16 21:32:29,826 - INFO - Device: cuda\n",
            "\n",
            "2025-11-16 21:32:29,826 - INFO - \n",
            "================================================================================\n",
            "2025-11-16 21:32:29,826 - INFO - Processing dolphin-llama-3.1-8b: cognitivecomputations/dolphin-2.9.4-llama3.1-8b\n",
            "2025-11-16 21:32:29,826 - INFO - ================================================================================\n",
            "\n",
            "2025-11-16 21:32:29,826 - INFO - Loading cognitivecomputations/dolphin-2.9.4-llama3.1-8b...\n",
            "tokenizer_config.json: 51.2kB [00:00, 22.0MB/s]\n",
            "tokenizer.json: 9.09MB [00:00, 146MB/s]\n",
            "special_tokens_map.json: 100% 454/454 [00:00<00:00, 3.82MB/s]\n",
            "config.json: 100% 904/904 [00:00<00:00, 7.60MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "2025-11-16 21:32:33.940498: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763328753.961197    6031 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763328753.966485    6031 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763328753.980183    6031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763328753.980206    6031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763328753.980210    6031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763328753.980216    6031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 21:32:33.984260: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors.index.json: 23.9kB [00:00, 73.6MB/s]\n",
            "Fetching 4 files:   0% 0/4 [00:00<?, ?it/s]\n",
            "model-00004-of-00004.safetensors:   0% 0.00/1.17G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   0% 0.00/4.98G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   0% 0.00/5.00G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   0% 0.00/4.92G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:   0% 534k/1.17G [00:02<1:19:15, 246kB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   1% 67.1M/4.92G [00:02<02:49, 28.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   0% 8.49M/5.00G [00:02<28:39, 2.90MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   2% 83.0M/4.92G [00:06<07:03, 11.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   1% 44.0M/5.00G [00:12<22:24, 3.69MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   4% 193M/4.92G [00:12<05:21, 14.7MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   3% 145M/5.00G [00:14<06:17, 12.9MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   6% 285M/4.92G [00:15<03:50, 20.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   4% 212M/5.00G [00:15<04:23, 18.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   7% 352M/4.92G [00:16<02:50, 26.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   6% 287M/5.00G [00:16<02:52, 27.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   9% 419M/4.92G [00:16<02:05, 35.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   8% 404M/5.00G [00:17<01:39, 46.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  10% 486M/4.92G [00:17<01:40, 43.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  10% 521M/5.00G [00:18<01:14, 60.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:   0% 534k/1.17G [00:18<1:19:15, 246kB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  11% 537M/4.92G [00:20<02:11, 33.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  11% 538M/5.00G [00:20<01:53, 39.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:   2% 28.4M/1.17G [00:20<13:36, 1.40MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  12% 604M/4.92G [00:20<01:43, 41.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:   8% 95.5M/1.17G [00:21<03:03, 5.84MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  12% 605M/5.00G [00:22<01:47, 41.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  14% 163M/1.17G [00:22<01:25, 11.7MB/s] \u001b[A\n",
            "model-00004-of-00004.safetensors:  20% 230M/1.17G [00:22<00:47, 19.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  14% 672M/4.92G [00:22<01:40, 42.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  13% 672M/5.00G [00:22<01:25, 50.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  25% 297M/1.17G [00:25<00:43, 19.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  15% 739M/4.92G [00:26<02:26, 28.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  31% 364M/1.17G [00:26<00:30, 26.1MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  37% 431M/1.17G [00:27<00:19, 37.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  16% 806M/4.92G [00:27<01:51, 37.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  43% 498M/1.17G [00:27<00:13, 50.5MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  48% 565M/1.17G [00:28<00:10, 57.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  18% 873M/4.92G [00:28<01:37, 41.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  54% 632M/1.17G [00:32<00:17, 30.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   0% 647k/4.98G [00:32<69:48:10, 19.8kB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  19% 940M/4.92G [00:32<02:26, 27.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  60% 699M/1.17G [00:32<00:11, 42.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  21% 1.01G/4.92G [00:33<01:48, 35.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  66% 766M/1.17G [00:33<00:07, 53.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  71% 833M/1.17G [00:34<00:05, 56.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  13% 672M/5.00G [00:38<01:25, 50.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  15% 739M/5.00G [00:38<06:08, 11.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  77% 900M/1.17G [00:38<00:08, 31.3MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  83% 967M/1.17G [00:39<00:04, 42.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  89% 1.03G/1.17G [00:44<00:05, 24.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  94% 1.10G/1.17G [00:44<00:02, 32.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  15% 753M/5.00G [00:44<08:11, 8.64MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   1% 67.7M/4.98G [00:44<42:38, 1.92MB/s]  \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  22% 1.07G/4.92G [00:45<04:43, 13.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors: 100% 1.17G/1.17G [00:45<00:00, 25.6MB/s]\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   3% 135M/4.98G [00:45<17:58, 4.49MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  15% 761M/5.00G [00:46<08:37, 8.19MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   4% 202M/4.98G [00:46<10:09, 7.83MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  16% 776M/5.00G [00:48<08:22, 8.41MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   7% 336M/4.98G [00:49<05:05, 15.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  17% 843M/5.00G [00:50<05:14, 13.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   9% 470M/4.98G [00:50<03:00, 24.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  22% 1.08G/4.92G [00:53<07:50, 8.15MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  18% 910M/5.00G [00:53<04:38, 14.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  11% 537M/4.98G [00:54<03:23, 21.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  20% 977M/5.00G [00:55<03:26, 19.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  13% 671M/4.98G [00:55<02:08, 33.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  22% 1.11G/5.00G [00:59<02:33, 25.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  15% 738M/4.98G [00:59<02:36, 27.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  25% 1.25G/5.00G [00:59<01:32, 40.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  16% 805M/4.98G [01:00<02:08, 32.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  26% 1.29G/5.00G [01:00<01:27, 42.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  19% 939M/4.98G [01:00<01:18, 51.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  26% 1.30G/5.00G [01:01<01:38, 37.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  23% 1.11G/4.92G [01:01<09:59, 6.35MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  20% 1.01G/4.98G [01:03<01:29, 44.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  26% 1.31G/5.00G [01:03<02:13, 27.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  23% 1.14G/4.98G [01:03<01:00, 63.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  28% 1.38G/5.00G [01:03<01:33, 38.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  24% 1.21G/4.98G [01:05<01:02, 60.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  29% 1.45G/5.00G [01:05<01:24, 42.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  23% 1.12G/4.92G [01:05<10:57, 5.78MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  32% 1.62G/5.00G [01:05<00:39, 85.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  23% 1.12G/4.92G [01:05<10:43, 5.89MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  26% 1.28G/4.98G [01:05<00:57, 64.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  33% 1.67G/5.00G [01:06<00:37, 87.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  27% 1.34G/4.98G [01:06<00:56, 64.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  36% 1.81G/5.00G [01:07<00:29, 110MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  28% 1.41G/4.98G [01:07<00:49, 72.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  24% 1.19G/4.92G [01:08<06:10, 10.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  30% 1.48G/4.98G [01:09<01:08, 51.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  26% 1.26G/4.92G [01:11<04:39, 13.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  38% 1.90G/5.00G [01:12<01:08, 45.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  29% 1.44G/4.92G [01:12<01:48, 32.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  31% 1.54G/4.98G [01:12<01:29, 38.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  39% 1.93G/5.00G [01:12<01:10, 43.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  30% 1.49G/4.92G [01:13<01:35, 35.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  34% 1.68G/4.98G [01:13<00:56, 58.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  39% 1.96G/5.00G [01:13<01:11, 42.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  41% 2.03G/5.00G [01:14<00:59, 50.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  35% 1.75G/4.98G [01:15<01:02, 51.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  44% 2.22G/5.00G [01:15<00:32, 85.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  31% 1.53G/4.92G [01:15<01:59, 28.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  46% 2.29G/5.00G [01:17<00:38, 70.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  32% 1.59G/4.92G [01:17<01:47, 30.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  37% 1.80G/4.92G [01:21<01:21, 38.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  36% 1.81G/4.98G [01:21<02:04, 25.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  47% 2.36G/5.00G [01:22<01:14, 35.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  39% 1.93G/4.92G [01:22<00:53, 55.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  40% 1.95G/4.92G [01:23<01:03, 46.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  39% 1.95G/4.98G [01:23<01:28, 34.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  48% 2.42G/5.00G [01:23<01:12, 35.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  40% 2.01G/4.98G [01:24<01:15, 39.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  41% 2.02G/4.92G [01:25<01:04, 45.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  44% 2.17G/4.92G [01:25<00:36, 75.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  42% 2.08G/4.98G [01:25<01:06, 43.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  46% 2.25G/4.92G [01:25<00:29, 90.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  43% 2.16G/4.98G [01:29<01:30, 31.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  47% 2.31G/4.92G [01:30<01:02, 41.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  45% 2.22G/4.98G [01:30<01:11, 38.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  48% 2.38G/4.92G [01:30<00:50, 49.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  50% 2.48G/5.00G [01:34<02:37, 16.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  47% 2.36G/4.98G [01:34<01:15, 34.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  50% 2.50G/5.00G [01:35<02:38, 15.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  49% 2.41G/4.92G [01:35<01:39, 25.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  49% 2.45G/4.98G [01:36<01:01, 41.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  51% 2.57G/5.00G [01:36<01:55, 21.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  51% 2.53G/4.92G [01:37<01:05, 36.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  52% 2.59G/5.00G [01:37<01:56, 20.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  51% 2.52G/4.98G [01:37<00:59, 41.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  53% 2.66G/5.00G [01:38<01:20, 29.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  52% 2.59G/4.98G [01:38<00:52, 45.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  56% 2.82G/5.00G [01:39<00:38, 57.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  52% 2.56G/4.92G [01:39<01:20, 29.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  53% 2.65G/4.98G [01:39<00:45, 51.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  58% 2.89G/5.00G [01:39<00:33, 63.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  55% 2.72G/4.98G [01:40<00:39, 57.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  53% 2.63G/4.92G [01:40<01:08, 33.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  57% 2.83G/4.98G [01:41<00:28, 75.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  56% 2.75G/4.92G [01:41<00:43, 50.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  58% 2.89G/4.98G [01:44<00:46, 45.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  59% 2.93G/5.00G [01:44<01:09, 29.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  57% 2.82G/4.92G [01:45<00:57, 36.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  60% 2.99G/5.00G [01:45<00:53, 37.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  59% 2.93G/4.98G [01:45<00:49, 41.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  60% 3.01G/4.98G [01:50<01:11, 27.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  61% 3.07G/5.00G [01:50<01:22, 23.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  58% 2.85G/4.92G [01:51<01:48, 19.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  61% 3.00G/4.92G [01:57<01:30, 21.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  62% 3.08G/4.98G [01:57<01:48, 17.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  63% 3.14G/5.00G [01:58<01:54, 16.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  64% 3.18G/4.98G [01:58<01:08, 26.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  65% 3.24G/5.00G [02:00<01:19, 22.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  62% 3.03G/4.92G [02:00<01:37, 19.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  65% 3.25G/4.98G [02:00<01:00, 28.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  64% 3.15G/4.92G [02:01<01:00, 29.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  65% 3.26G/5.00G [02:01<01:22, 21.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  67% 3.32G/4.98G [02:01<00:51, 32.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  67% 3.33G/5.00G [02:02<00:58, 28.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  65% 3.22G/4.92G [02:02<00:50, 33.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  68% 3.39G/4.98G [02:02<00:42, 37.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  67% 3.28G/4.92G [02:03<00:42, 38.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  69% 3.46G/4.98G [02:03<00:34, 44.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  71% 3.48G/4.92G [02:05<00:24, 59.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  71% 3.52G/4.98G [02:05<00:34, 42.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  67% 3.37G/5.00G [02:05<01:16, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  71% 3.51G/4.92G [02:06<00:27, 51.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  72% 3.59G/4.98G [02:06<00:31, 44.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  73% 3.58G/4.92G [02:07<00:23, 55.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  75% 3.70G/4.92G [02:11<00:29, 41.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  69% 3.43G/5.00G [02:12<01:43, 15.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  77% 3.79G/4.92G [02:12<00:23, 47.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  73% 3.66G/4.98G [02:12<00:56, 23.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  78% 3.85G/4.92G [02:13<00:21, 48.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  75% 3.72G/4.98G [02:13<00:43, 29.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  76% 3.79G/4.98G [02:16<00:44, 27.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  82% 4.02G/4.92G [02:16<00:18, 49.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  77% 3.85G/4.98G [02:17<00:32, 34.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  83% 4.08G/4.92G [02:17<00:14, 56.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  78% 3.87G/4.98G [02:18<00:33, 33.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  84% 4.15G/4.92G [02:18<00:12, 62.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  69% 3.45G/5.00G [02:18<02:33, 10.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  79% 3.91G/4.98G [02:18<00:27, 38.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  70% 3.48G/5.00G [02:19<02:05, 12.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  72% 3.59G/5.00G [02:19<00:55, 25.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  86% 4.20G/4.92G [02:23<00:22, 31.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  87% 4.27G/4.92G [02:23<00:16, 40.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  73% 3.65G/5.00G [02:24<01:08, 19.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  81% 4.03G/4.98G [02:24<00:35, 26.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  74% 3.72G/5.00G [02:24<00:46, 27.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  88% 4.34G/4.92G [02:24<00:13, 41.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  82% 4.09G/4.98G [02:25<00:26, 32.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  90% 4.40G/4.92G [02:26<00:13, 39.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  91% 4.48G/4.92G [02:27<00:08, 52.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  92% 4.54G/4.92G [02:27<00:05, 65.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  83% 4.11G/4.98G [02:27<00:37, 23.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  76% 3.79G/5.00G [02:27<00:48, 25.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  76% 3.82G/5.00G [02:32<01:08, 17.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  77% 3.85G/5.00G [02:32<00:55, 20.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  84% 4.17G/4.98G [02:32<00:43, 18.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  94% 4.61G/4.92G [02:33<00:10, 28.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  85% 4.24G/4.98G [02:33<00:28, 25.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  85% 4.25G/4.98G [02:33<00:27, 26.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  86% 4.28G/4.98G [02:33<00:21, 32.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  78% 3.91G/5.00G [02:33<00:40, 26.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  79% 3.97G/5.00G [02:34<00:28, 36.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  81% 4.04G/5.00G [02:37<00:35, 27.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  95% 4.65G/4.92G [02:38<00:14, 18.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  87% 4.34G/4.98G [02:38<00:29, 21.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  82% 4.08G/5.00G [02:38<00:29, 30.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  83% 4.15G/5.00G [02:38<00:18, 45.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  84% 4.22G/5.00G [02:39<00:12, 64.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  86% 4.28G/5.00G [02:39<00:09, 72.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  96% 4.71G/4.92G [02:39<00:08, 23.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  97% 4.75G/4.92G [02:39<00:05, 28.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  89% 4.41G/4.98G [02:40<00:22, 25.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  87% 4.35G/5.00G [02:43<00:18, 35.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  90% 4.48G/4.98G [02:43<00:22, 22.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  91% 4.51G/4.98G [02:44<00:17, 26.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  88% 4.42G/5.00G [02:44<00:12, 45.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  98% 4.82G/4.92G [02:47<00:05, 16.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  90% 4.49G/5.00G [02:49<00:21, 24.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  92% 4.57G/4.98G [02:50<00:23, 17.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  93% 4.64G/4.98G [02:50<00:13, 25.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  95% 4.71G/4.98G [02:55<00:14, 18.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  91% 4.55G/5.00G [02:55<00:25, 17.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  92% 4.62G/5.00G [02:56<00:15, 25.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  99% 4.87G/4.92G [02:56<00:04, 10.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  94% 4.69G/5.00G [02:56<00:09, 32.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  95% 4.75G/5.00G [02:59<00:07, 31.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  96% 4.80G/5.00G [03:02<00:08, 25.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  97% 4.87G/5.00G [03:02<00:03, 35.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  96% 4.78G/4.98G [03:02<00:13, 14.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  97% 4.84G/4.98G [03:02<00:06, 21.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  99% 4.93G/5.00G [03:03<00:01, 43.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors: 100% 4.92G/4.92G [03:06<00:00, 26.4MB/s]\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  99% 4.91G/4.98G [03:06<00:03, 20.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors: 100% 4.98G/4.98G [03:06<00:00, 26.7MB/s]\n",
            "Fetching 4 files:  25% 1/4 [03:06<09:20, 186.83s/it]\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors: 100% 5.00G/5.00G [03:11<00:00, 26.2MB/s]\n",
            "Fetching 4 files: 100% 4/4 [03:11<00:00, 47.86s/it]\n",
            "2025-11-16 21:35:53,454 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
            "Loading checkpoint shards: 100% 4/4 [01:07<00:00, 16.93s/it]\n",
            "generation_config.json: 100% 211/211 [00:00<00:00, 1.58MB/s]\n",
            "2025-11-16 21:37:01,955 - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "2025-11-16 21:37:01,957 - INFO - ✓ Loaded cognitivecomputations/dolphin-2.9.4-llama3.1-8b on cuda\n",
            "2025-11-16 21:37:01,957 - INFO - Loading train and test datasets...\n",
            "2025-11-16 21:37:01,962 - INFO - Train: 35 pairs, Test: 15 pairs\n",
            "2025-11-16 21:37:01,962 - INFO - \n",
            "--- Layer 8 ---\n",
            "2025-11-16 21:37:01,962 - INFO - Extracting training activations...\n",
            "Layer 8: 100% 35/35 [00:39<00:00,  1.14s/it]\n",
            "Layer 8: 100% 35/35 [00:37<00:00,  1.09s/it]\n",
            "2025-11-16 21:38:19,748 - INFO - Computing probe direction...\n",
            "2025-11-16 21:38:19,749 - INFO - Extracting test activations...\n",
            "Layer 8: 100% 15/15 [00:16<00:00,  1.12s/it]\n",
            "Layer 8: 100% 15/15 [00:17<00:00,  1.14s/it]\n",
            "2025-11-16 21:38:53,608 - INFO - Validating probe...\n",
            "2025-11-16 21:38:53,633 - INFO - AUROC: 0.996, Accuracy: 96.7%\n",
            "2025-11-16 21:38:53,633 - INFO - Saved probe to /content/empathy-probes/results/cross_model_validation/dolphin-llama-3.1-8b_layer8_probe.npy\n",
            "2025-11-16 21:38:53,635 - INFO - \n",
            "--- Layer 12 ---\n",
            "2025-11-16 21:38:53,635 - INFO - Extracting training activations...\n",
            "Layer 12: 100% 35/35 [00:40<00:00,  1.15s/it]\n",
            "Layer 12: 100% 35/35 [00:38<00:00,  1.10s/it]\n",
            "2025-11-16 21:40:12,361 - INFO - Computing probe direction...\n",
            "2025-11-16 21:40:12,362 - INFO - Extracting test activations...\n",
            "Layer 12: 100% 15/15 [00:16<00:00,  1.13s/it]\n",
            "Layer 12: 100% 15/15 [00:17<00:00,  1.14s/it]\n",
            "2025-11-16 21:40:46,445 - INFO - Validating probe...\n",
            "2025-11-16 21:40:46,449 - INFO - AUROC: 0.996, Accuracy: 96.7%\n",
            "2025-11-16 21:40:46,449 - INFO - Saved probe to /content/empathy-probes/results/cross_model_validation/dolphin-llama-3.1-8b_layer12_probe.npy\n",
            "2025-11-16 21:40:46,449 - INFO - \n",
            "--- Layer 16 ---\n",
            "2025-11-16 21:40:46,450 - INFO - Extracting training activations...\n",
            "Layer 16: 100% 35/35 [00:39<00:00,  1.14s/it]\n",
            "Layer 16: 100% 35/35 [00:38<00:00,  1.10s/it]\n",
            "2025-11-16 21:42:04,968 - INFO - Computing probe direction...\n",
            "2025-11-16 21:42:04,969 - INFO - Extracting test activations...\n",
            "Layer 16: 100% 15/15 [00:17<00:00,  1.14s/it]\n",
            "Layer 16: 100% 15/15 [00:17<00:00,  1.14s/it]\n",
            "2025-11-16 21:42:39,196 - INFO - Validating probe...\n",
            "2025-11-16 21:42:39,200 - INFO - AUROC: 0.982, Accuracy: 96.7%\n",
            "2025-11-16 21:42:39,200 - INFO - Saved probe to /content/empathy-probes/results/cross_model_validation/dolphin-llama-3.1-8b_layer16_probe.npy\n",
            "2025-11-16 21:42:39,200 - INFO - \n",
            "--- Layer 20 ---\n",
            "2025-11-16 21:42:39,200 - INFO - Extracting training activations...\n",
            "Layer 20: 100% 35/35 [00:39<00:00,  1.14s/it]\n",
            "Layer 20: 100% 35/35 [00:38<00:00,  1.10s/it]\n",
            "2025-11-16 21:43:57,755 - INFO - Computing probe direction...\n",
            "2025-11-16 21:43:57,756 - INFO - Extracting test activations...\n",
            "Layer 20: 100% 15/15 [00:17<00:00,  1.14s/it]\n",
            "Layer 20: 100% 15/15 [00:17<00:00,  1.13s/it]\n",
            "2025-11-16 21:44:31,866 - INFO - Validating probe...\n",
            "2025-11-16 21:44:31,870 - INFO - AUROC: 0.969, Accuracy: 93.3%\n",
            "2025-11-16 21:44:31,870 - INFO - Saved probe to /content/empathy-probes/results/cross_model_validation/dolphin-llama-3.1-8b_layer20_probe.npy\n",
            "2025-11-16 21:44:31,870 - INFO - \n",
            "--- Layer 24 ---\n",
            "2025-11-16 21:44:31,870 - INFO - Extracting training activations...\n",
            "Layer 24: 100% 35/35 [00:39<00:00,  1.14s/it]\n",
            "Layer 24: 100% 35/35 [00:38<00:00,  1.11s/it]\n",
            "2025-11-16 21:45:50,545 - INFO - Computing probe direction...\n",
            "2025-11-16 21:45:50,547 - INFO - Extracting test activations...\n",
            "Layer 24: 100% 15/15 [00:16<00:00,  1.13s/it]\n",
            "Layer 24: 100% 15/15 [00:17<00:00,  1.14s/it]\n",
            "2025-11-16 21:46:24,562 - INFO - Validating probe...\n",
            "2025-11-16 21:46:24,567 - INFO - AUROC: 0.969, Accuracy: 93.3%\n",
            "2025-11-16 21:46:24,568 - INFO - Saved probe to /content/empathy-probes/results/cross_model_validation/dolphin-llama-3.1-8b_layer24_probe.npy\n",
            "2025-11-16 21:46:24,568 - INFO - \n",
            "--- Layer 28 ---\n",
            "2025-11-16 21:46:24,568 - INFO - Extracting training activations...\n",
            "Layer 28: 100% 35/35 [00:40<00:00,  1.14s/it]\n",
            "Layer 28: 100% 35/35 [00:38<00:00,  1.10s/it]\n",
            "2025-11-16 21:47:43,270 - INFO - Computing probe direction...\n",
            "2025-11-16 21:47:43,272 - INFO - Extracting test activations...\n",
            "Layer 28: 100% 15/15 [00:16<00:00,  1.13s/it]\n",
            "Layer 28: 100% 15/15 [00:17<00:00,  1.14s/it]\n",
            "2025-11-16 21:48:17,305 - INFO - Validating probe...\n",
            "2025-11-16 21:48:17,308 - INFO - AUROC: 0.978, Accuracy: 93.3%\n",
            "2025-11-16 21:48:17,308 - INFO - Saved probe to /content/empathy-probes/results/cross_model_validation/dolphin-llama-3.1-8b_layer28_probe.npy\n",
            "2025-11-16 21:48:17,309 - INFO - \n",
            "✓ Completed dolphin-llama-3.1-8b\n",
            "2025-11-16 21:48:17,309 - INFO - Results saved to /content/empathy-probes/results/cross_model_validation/dolphin-llama-3.1-8b_results.json\n",
            "\n",
            "2025-11-16 21:48:17,350 - INFO - \n",
            "================================================================================\n",
            "2025-11-16 21:48:17,350 - INFO - All models completed!\n",
            "2025-11-16 21:48:17,350 - INFO - Combined results saved to /content/empathy-probes/results/cross_model_validation/all_models_results.json\n",
            "2025-11-16 21:48:17,350 - INFO - ================================================================================\n",
            "\n",
            "\n",
            "=== SUMMARY: Cross-Model Probe Performance ===\n",
            "\n",
            "Model                     Layer    AUROC    Accuracy  \n",
            "-------------------------------------------------------\n",
            "dolphin-llama-3.1-8b      8        0.996    96.7%     \n",
            "dolphin-llama-3.1-8b      12       0.996    96.7%     \n",
            "dolphin-llama-3.1-8b      16       0.982    96.7%     \n",
            "dolphin-llama-3.1-8b      20       0.969    93.3%     \n",
            "dolphin-llama-3.1-8b      24       0.969    93.3%     \n",
            "dolphin-llama-3.1-8b      28       0.978    93.3%     \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "with open('results/cross_model_validation/all_models_results.json') as f:\n",
        "  results = json.load(f)\n",
        "\n",
        "rows = []\n",
        "for model, data in results.items():\n",
        "    for layer, metrics in data['layers'].items():\n",
        "        rows.append({\n",
        "            'Model': model,\n",
        "            'Layer': layer,\n",
        "            'AUROC': f\"{metrics['auroc']:.3f}\",\n",
        "            'Accuracy': f\"{metrics['accuracy']:.1%}\"\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "print(\"\\n=== Results ===\\n\")\n",
        "print(df.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSCJ-1pWgvxx",
        "outputId": "aff8cb89-894e-4e6c-972b-0b3cc3543da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Results ===\n",
            "\n",
            "               Model Layer AUROC Accuracy\n",
            "dolphin-llama-3.1-8b     8 0.996    96.7%\n",
            "dolphin-llama-3.1-8b    12 0.996    96.7%\n",
            "dolphin-llama-3.1-8b    16 0.982    96.7%\n",
            "dolphin-llama-3.1-8b    20 0.969    93.3%\n",
            "dolphin-llama-3.1-8b    24 0.969    93.3%\n",
            "dolphin-llama-3.1-8b    28 0.978    93.3%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "with open('results/cross_model_validation/all_models_results.json') as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "rows = []\n",
        "for model, data in results.items():\n",
        "    for layer, metrics in data['layers'].items():\n",
        "        rows.append({\n",
        "            'Model': model,\n",
        "            'Layer': layer,\n",
        "            'AUROC': f\"{metrics['auroc']:.3f}\",\n",
        "            'Accuracy': f\"{metrics['accuracy']:.1%}\"\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "print(\"\\n=== ALL MODELS ===\\n\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "# Compare best layers\n",
        "print(\"\\n=== Best Layer Per Model ===\\n\")\n",
        "best = df.loc[df.groupby('Model')['AUROC'].idxmax()]\n",
        "print(best[['Model', 'Layer', 'AUROC',\n",
        "'Accuracy']].to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0610ZwIKpZzI",
        "outputId": "1e8a7565-1c74-47ce-ed55-9163bb12b85b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== ALL MODELS ===\n",
            "\n",
            "               Model Layer AUROC Accuracy\n",
            "dolphin-llama-3.1-8b     8 0.996    96.7%\n",
            "dolphin-llama-3.1-8b    12 0.996    96.7%\n",
            "dolphin-llama-3.1-8b    16 0.982    96.7%\n",
            "dolphin-llama-3.1-8b    20 0.969    93.3%\n",
            "dolphin-llama-3.1-8b    24 0.969    93.3%\n",
            "dolphin-llama-3.1-8b    28 0.978    93.3%\n",
            "\n",
            "=== Best Layer Per Model ===\n",
            "\n",
            "               Model Layer AUROC Accuracy\n",
            "dolphin-llama-3.1-8b     8 0.996    96.7%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  !zip -r results.zip results/cross_model_validation/\n",
        "  from google.colab import files\n",
        "  files.download('results.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "1Z139bPogw8r",
        "outputId": "2246c02c-0cc5-46e7-e3e5-adb582552e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: results/cross_model_validation/ (stored 0%)\n",
            "  adding: results/cross_model_validation/qwen2.5-7b_results.json (deflated 74%)\n",
            "  adding: results/cross_model_validation/qwen2.5-7b_layer24_probe.npy (deflated 8%)\n",
            "  adding: results/cross_model_validation/qwen2.5-7b_layer20_probe.npy (deflated 7%)\n",
            "  adding: results/cross_model_validation/qwen2.5-7b_layer16_probe.npy (deflated 7%)\n",
            "  adding: results/cross_model_validation/dolphin-llama-3.1-8b_layer12_probe.npy (deflated 8%)\n",
            "  adding: results/cross_model_validation/dolphin-llama-3.1-8b_layer16_probe.npy (deflated 8%)\n",
            "  adding: results/cross_model_validation/dolphin-llama-3.1-8b_results.json (deflated 76%)\n",
            "  adding: results/cross_model_validation/dolphin-llama-3.1-8b_layer20_probe.npy (deflated 8%)\n",
            "  adding: results/cross_model_validation/dolphin-llama-3.1-8b_layer8_probe.npy (deflated 8%)\n",
            "  adding: results/cross_model_validation/qwen2.5-7b_layer8_probe.npy (deflated 7%)\n",
            "  adding: results/cross_model_validation/dolphin-llama-3.1-8b_layer24_probe.npy (deflated 8%)\n",
            "  adding: results/cross_model_validation/qwen2.5-7b_layer12_probe.npy (deflated 8%)\n",
            "  adding: results/cross_model_validation/dolphin-llama-3.1-8b_layer28_probe.npy (deflated 8%)\n",
            "  adding: results/cross_model_validation/all_models_results.json (deflated 77%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b5b969b0-4d93-496c-ad17-4b9549e15ee3\", \"results.zip\", 84731)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}