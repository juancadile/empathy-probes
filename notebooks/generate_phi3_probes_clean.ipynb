{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate Phi-3 Empathy Probes\n",
        "\n",
        "This notebook extracts empathy probe directions from Phi-3-mini-4k-instruct.\n",
        "\n",
        "**Run cells in order!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Install packages\n",
        "!pip install torch transformers==4.41.0 accelerate einops scikit-learn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Import everything\n",
        "import torch\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import requests\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from typing import List, Dict\n",
        "import gc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "print(\"Imports complete ✓\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Setup device and load model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "# Load model\n",
        "print(\"\\nLoading Phi-3 model...\")\n",
        "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"eager\"\n",
        ")\n",
        "print(f\"Model loaded! Parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.1f}B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Download contrastive data\n",
        "print(\"Downloading contrastive data from GitHub...\")\n",
        "url = \"https://raw.githubusercontent.com/juancadile/empathy-probes/main/data/contrastive_pairs/train_pairs.jsonl\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        lines = response.text.strip().split('\\n')\n",
        "        contrastive_data = []\n",
        "        for line in lines:\n",
        "            if line:\n",
        "                pair = json.loads(line)\n",
        "                contrastive_data.append({\n",
        "                    \"empathic\": pair.get(\"empathetic\", \"\"),\n",
        "                    \"non_empathic\": pair.get(\"non_empathetic\", \"\")\n",
        "                })\n",
        "        print(f\"✓ Downloaded {len(contrastive_data)} pairs\")\n",
        "    else:\n",
        "        print(f\"Failed to download: Status {response.status_code}\")\n",
        "        raise Exception(\"Download failed\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"\\nUsing fallback data (only 2 pairs - results will be poor!)\")\n",
        "    contrastive_data = [\n",
        "        {\n",
        "            \"empathic\": \"I understand you're struggling. Let me help you.\",\n",
        "            \"non_empathic\": \"Complete the task efficiently.\"\n",
        "        },\n",
        "        {\n",
        "            \"empathic\": \"I can see this is difficult for you.\",\n",
        "            \"non_empathic\": \"Proceed to the next step.\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "print(f\"Total pairs available: {len(contrastive_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Define probe extraction functions\n",
        "\n",
        "def extract_activations(model, tokenizer, text: str, layer: int, device) -> torch.Tensor:\n",
        "    \"\"\"Extract activations from a specific layer.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "    \n",
        "    activations = None\n",
        "    \n",
        "    def hook(module, input, output):\n",
        "        nonlocal activations\n",
        "        if isinstance(output, tuple):\n",
        "            activations = output[0]\n",
        "        else:\n",
        "            activations = output\n",
        "    \n",
        "    # Register hook\n",
        "    hook_handle = model.model.layers[layer].register_forward_hook(hook)\n",
        "    \n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        _ = model(**inputs)\n",
        "    \n",
        "    # Remove hook\n",
        "    hook_handle.remove()\n",
        "    \n",
        "    # Mean pool across sequence length\n",
        "    return activations.mean(dim=1).squeeze().cpu()\n",
        "\n",
        "\n",
        "def compute_probe_direction(empathic_texts: List[str], \n",
        "                           non_empathic_texts: List[str], \n",
        "                           layer: int,\n",
        "                           model, tokenizer, device) -> Dict:\n",
        "    \"\"\"Compute probe direction from contrastive pairs.\"\"\"\n",
        "    \n",
        "    empathic_acts = []\n",
        "    non_empathic_acts = []\n",
        "    \n",
        "    print(f\"Extracting activations for layer {layer}...\")\n",
        "    \n",
        "    # Extract activations\n",
        "    for i, (emp_text, non_text) in enumerate(zip(empathic_texts, non_empathic_texts)):\n",
        "        if i % 5 == 0:\n",
        "            print(f\"  Processing pair {i+1}/{len(empathic_texts)}...\")\n",
        "        \n",
        "        emp_act = extract_activations(model, tokenizer, emp_text, layer, device)\n",
        "        non_act = extract_activations(model, tokenizer, non_text, layer, device)\n",
        "        \n",
        "        empathic_acts.append(emp_act)\n",
        "        non_empathic_acts.append(non_act)\n",
        "        \n",
        "        # Clear cache periodically\n",
        "        if i % 10 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "    \n",
        "    # Stack and compute means\n",
        "    empathic_acts = torch.stack(empathic_acts)\n",
        "    non_empathic_acts = torch.stack(non_empathic_acts)\n",
        "    \n",
        "    emp_mean = empathic_acts.mean(dim=0)\n",
        "    non_mean = non_empathic_acts.mean(dim=0)\n",
        "    \n",
        "    # Compute probe direction\n",
        "    probe_direction = emp_mean - non_mean\n",
        "    probe_direction = probe_direction / probe_direction.norm()\n",
        "    \n",
        "    # Compute statistics\n",
        "    emp_projections = (empathic_acts @ probe_direction).numpy()\n",
        "    non_projections = (non_empathic_acts @ probe_direction).numpy()\n",
        "    \n",
        "    # AUROC\n",
        "    labels = [1] * len(emp_projections) + [0] * len(non_projections)\n",
        "    scores = np.concatenate([emp_projections, non_projections])\n",
        "    auroc = roc_auc_score(labels, scores)\n",
        "    \n",
        "    # Accuracy\n",
        "    threshold = (emp_projections.mean() + non_projections.mean()) / 2\n",
        "    emp_correct = (emp_projections > threshold).sum()\n",
        "    non_correct = (non_projections <= threshold).sum()\n",
        "    accuracy = (emp_correct + non_correct) / (len(emp_projections) + len(non_projections))\n",
        "    \n",
        "    return {\n",
        "        \"layer\": layer,\n",
        "        \"probe_direction\": probe_direction.numpy(),\n",
        "        \"empathic_mean\": emp_mean.numpy(),\n",
        "        \"non_empathic_mean\": non_mean.numpy(),\n",
        "        \"auroc\": float(auroc),\n",
        "        \"accuracy\": float(accuracy),\n",
        "        \"threshold\": float(threshold),\n",
        "        \"separation\": float(emp_projections.mean() - non_projections.mean())\n",
        "    }\n",
        "\n",
        "print(\"Functions defined ✓\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Extract probes for all layers\n",
        "layers_to_test = [8, 12, 16, 20, 24]\n",
        "\n",
        "# Prepare texts\n",
        "empathic_texts = [pair[\"empathic\"] for pair in contrastive_data]\n",
        "non_empathic_texts = [pair[\"non_empathic\"] for pair in contrastive_data]\n",
        "\n",
        "# Use first 35 pairs for training (or all if less)\n",
        "train_size = min(35, len(empathic_texts))\n",
        "empathic_train = empathic_texts[:train_size]\n",
        "non_empathic_train = non_empathic_texts[:train_size]\n",
        "\n",
        "print(f\"Using {train_size} contrastive pairs for probe extraction\")\n",
        "print(f\"Testing layers: {layers_to_test}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "results = {}\n",
        "for layer in layers_to_test:\n",
        "    print(f\"\\nLayer {layer}:\")\n",
        "    print(\"-\"*20)\n",
        "    \n",
        "    probe_data = compute_probe_direction(\n",
        "        empathic_train, non_empathic_train, layer,\n",
        "        model, tokenizer, device\n",
        "    )\n",
        "    \n",
        "    print(f\"  AUROC: {probe_data['auroc']:.3f}\")\n",
        "    print(f\"  Accuracy: {probe_data['accuracy']:.3f}\")\n",
        "    print(f\"  Separation: {probe_data['separation']:.3f}\")\n",
        "    \n",
        "    # Save probe\n",
        "    filename = f\"phi3_layer_{layer}_validation.pkl\"\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(probe_data, f)\n",
        "    print(f\"  Saved: {filename}\")\n",
        "    \n",
        "    results[f\"layer_{layer}\"] = probe_data\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "for layer in layers_to_test:\n",
        "    data = results[f\"layer_{layer}\"]\n",
        "    print(f\"Layer {layer}: AUROC={data['auroc']:.3f}, Acc={data['accuracy']:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Download probe files\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Downloading probe files...\")\n",
        "for layer in layers_to_test:\n",
        "    filename = f\"phi3_layer_{layer}_validation.pkl\"\n",
        "    try:\n",
        "        files.download(filename)\n",
        "        print(f\"✓ {filename}\")\n",
        "    except:\n",
        "        print(f\"✗ {filename} not found\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}