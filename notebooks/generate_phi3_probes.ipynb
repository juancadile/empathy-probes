{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Phi-3 Empathy Probes\n",
    "\n",
    "This notebook extracts empathy probe directions from Phi-3-mini-4k-instruct.\n",
    "\n",
    "**Run cells in order!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install packages\n",
    "!pip install torch transformers==4.41.0 accelerate einops scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import everything\n",
    "import torch\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import requests\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import List, Dict\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"Imports complete ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Setup device and load model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Load model\n",
    "print(\"\\nLoading Phi-3 model...\")\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "print(f\"Model loaded! Parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.1f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Download contrastive data\nprint(\"Downloading contrastive data from GitHub...\")\nurl = \"https://raw.githubusercontent.com/juancadile/empathy-probes/main/data/contrastive_pairs/train_pairs.jsonl\"\n\ntry:\n    response = requests.get(url)\n    if response.status_code == 200:\n        lines = response.text.strip().split('\\n')\n        contrastive_data = []\n        for line in lines:\n            if line.strip():  # Skip empty lines\n                try:\n                    pair = json.loads(line)\n                    # Try different field names\n                    emp_text = (pair.get(\"empathic_text\", \"\") or \n                               pair.get(\"empathetic\", \"\") or \n                               pair.get(\"empathic\", \"\")).strip()\n                    non_text = (pair.get(\"non_empathic_text\", \"\") or \n                               pair.get(\"non_empathetic\", \"\") or \n                               pair.get(\"non_empathic\", \"\")).strip()\n                    \n                    # Only add if both texts are non-empty\n                    if emp_text and non_text:\n                        contrastive_data.append({\n                            \"empathic\": emp_text,\n                            \"non_empathic\": non_text\n                        })\n                except json.JSONDecodeError as e:\n                    print(f\"Skipping line due to JSON error: {e}\")\n                    continue\n        print(f\"✓ Downloaded {len(contrastive_data)} valid pairs\")\n    else:\n        print(f\"Failed to download: Status {response.status_code}\")\n        raise Exception(\"Download failed\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n    print(\"\\nUsing fallback data (only 2 pairs - results will be poor!)\")\n    contrastive_data = [\n        {\n            \"empathic\": \"I understand you're struggling and need help. Let me assist you with care and support.\",\n            \"non_empathic\": \"Complete the task efficiently. Focus on the objective and proceed.\"\n        },\n        {\n            \"empathic\": \"I can see this is difficult for you. Take your time, I'm here to help.\",\n            \"non_empathic\": \"Proceed to the next step. Execute the command as instructed.\"\n        }\n    ]\n\nprint(f\"Total valid pairs: {len(contrastive_data)}\")\n\n# Verify we have data\nif len(contrastive_data) == 0:\n    print(\"ERROR: No valid contrastive pairs found!\")\n    print(\"Please check the data format\")\nelse:\n    print(f\"\\nSample pair:\")\n    print(f\"  Empathic: {contrastive_data[0]['empathic'][:80]}...\")\n    print(f\"  Non-empathic: {contrastive_data[0]['non_empathic'][:80]}...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Define probe extraction functions\n\ndef extract_activations(model, tokenizer, text: str, layer: int, device) -> torch.Tensor:\n    \"\"\"Extract activations from a specific layer.\"\"\"\n    # Ensure text is not empty\n    if not text or not text.strip():\n        print(f\"Warning: Empty text provided, using placeholder\")\n        text = \"This is a placeholder text.\"\n    \n    # Tokenize with proper settings for Phi-3\n    inputs = tokenizer(\n        text, \n        return_tensors=\"pt\", \n        max_length=512, \n        truncation=True,\n        padding=True,  # Add padding\n        return_attention_mask=True\n    )\n    \n    # Move to device\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    # Check if we got valid tokens\n    if inputs['input_ids'].shape[1] == 0:\n        print(f\"Warning: Tokenization resulted in empty sequence for text: {text[:50]}...\")\n        # Create a minimal valid input\n        inputs['input_ids'] = torch.tensor([[1]], device=device)  # Use a valid token ID\n        inputs['attention_mask'] = torch.tensor([[1]], device=device)\n    \n    activations = None\n    \n    def hook(module, input, output):\n        nonlocal activations\n        if isinstance(output, tuple):\n            activations = output[0]\n        else:\n            activations = output\n    \n    # Register hook\n    hook_handle = model.model.layers[layer].register_forward_hook(hook)\n    \n    # Forward pass\n    try:\n        with torch.no_grad():\n            _ = model(**inputs)\n    except Exception as e:\n        print(f\"Error during forward pass: {e}\")\n        print(f\"Input shape: {inputs['input_ids'].shape}\")\n        raise\n    finally:\n        # Always remove hook\n        hook_handle.remove()\n    \n    # Mean pool across sequence length\n    if activations is not None:\n        return activations.mean(dim=1).squeeze().cpu()\n    else:\n        raise ValueError(\"No activations captured\")\n\n\ndef compute_probe_direction(empathic_texts: List[str], \n                           non_empathic_texts: List[str], \n                           layer: int,\n                           model, tokenizer, device) -> Dict:\n    \"\"\"Compute probe direction from contrastive pairs.\"\"\"\n    \n    empathic_acts = []\n    non_empathic_acts = []\n    valid_pairs = 0\n    \n    print(f\"Extracting activations for layer {layer}...\")\n    \n    # Extract activations\n    for i, (emp_text, non_text) in enumerate(zip(empathic_texts, non_empathic_texts)):\n        if i % 5 == 0:\n            print(f\"  Processing pair {i+1}/{len(empathic_texts)}...\")\n        \n        try:\n            emp_act = extract_activations(model, tokenizer, emp_text, layer, device)\n            non_act = extract_activations(model, tokenizer, non_text, layer, device)\n            \n            empathic_acts.append(emp_act)\n            non_empathic_acts.append(non_act)\n            valid_pairs += 1\n        except Exception as e:\n            print(f\"  Skipping pair {i+1} due to error: {e}\")\n            continue\n        \n        # Clear cache periodically\n        if i % 10 == 0:\n            torch.cuda.empty_cache()\n    \n    if valid_pairs == 0:\n        raise ValueError(\"No valid pairs processed!\")\n    \n    print(f\"  Processed {valid_pairs}/{len(empathic_texts)} valid pairs\")\n    \n    # Stack and compute means\n    empathic_acts = torch.stack(empathic_acts)\n    non_empathic_acts = torch.stack(non_empathic_acts)\n    \n    emp_mean = empathic_acts.mean(dim=0)\n    non_mean = non_empathic_acts.mean(dim=0)\n    \n    # Compute probe direction\n    probe_direction = emp_mean - non_mean\n    probe_norm = probe_direction.norm()\n    \n    if probe_norm > 0:\n        probe_direction = probe_direction / probe_norm\n    else:\n        print(\"Warning: Zero probe direction, using random direction\")\n        probe_direction = torch.randn_like(probe_direction)\n        probe_direction = probe_direction / probe_direction.norm()\n    \n    # Compute statistics\n    emp_projections = (empathic_acts @ probe_direction).numpy()\n    non_projections = (non_empathic_acts @ probe_direction).numpy()\n    \n    # AUROC\n    from sklearn.metrics import roc_auc_score\n    labels = [1] * len(emp_projections) + [0] * len(non_projections)\n    scores = np.concatenate([emp_projections, non_projections])\n    \n    try:\n        auroc = roc_auc_score(labels, scores)\n    except:\n        auroc = 0.5  # Default if AUROC fails\n    \n    # Accuracy\n    threshold = (emp_projections.mean() + non_projections.mean()) / 2\n    emp_correct = (emp_projections > threshold).sum()\n    non_correct = (non_projections <= threshold).sum()\n    accuracy = (emp_correct + non_correct) / (len(emp_projections) + len(non_projections))\n    \n    return {\n        \"layer\": layer,\n        \"probe_direction\": probe_direction.numpy(),\n        \"empathic_mean\": emp_mean.numpy(),\n        \"non_empathic_mean\": non_mean.numpy(),\n        \"auroc\": float(auroc),\n        \"accuracy\": float(accuracy),\n        \"threshold\": float(threshold),\n        \"separation\": float(emp_projections.mean() - non_projections.mean()),\n        \"valid_pairs\": valid_pairs\n    }\n\nprint(\"Functions defined ✓\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Extract probes for all layers\n",
    "layers_to_test = [8, 12, 16, 20, 24]\n",
    "\n",
    "# Prepare texts\n",
    "empathic_texts = [pair[\"empathic\"] for pair in contrastive_data]\n",
    "non_empathic_texts = [pair[\"non_empathic\"] for pair in contrastive_data]\n",
    "\n",
    "# Use first 35 pairs for training (or all if less)\n",
    "train_size = min(35, len(empathic_texts))\n",
    "empathic_train = empathic_texts[:train_size]\n",
    "non_empathic_train = non_empathic_texts[:train_size]\n",
    "\n",
    "print(f\"Using {train_size} contrastive pairs for probe extraction\")\n",
    "print(f\"Testing layers: {layers_to_test}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "results = {}\n",
    "for layer in layers_to_test:\n",
    "    print(f\"\\nLayer {layer}:\")\n",
    "    print(\"-\"*20)\n",
    "    \n",
    "    probe_data = compute_probe_direction(\n",
    "        empathic_train, non_empathic_train, layer,\n",
    "        model, tokenizer, device\n",
    "    )\n",
    "    \n",
    "    print(f\"  AUROC: {probe_data['auroc']:.3f}\")\n",
    "    print(f\"  Accuracy: {probe_data['accuracy']:.3f}\")\n",
    "    print(f\"  Separation: {probe_data['separation']:.3f}\")\n",
    "    \n",
    "    # Save probe\n",
    "    filename = f\"phi3_layer_{layer}_validation.pkl\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(probe_data, f)\n",
    "    print(f\"  Saved: {filename}\")\n",
    "    \n",
    "    results[f\"layer_{layer}\"] = probe_data\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for layer in layers_to_test:\n",
    "    data = results[f\"layer_{layer}\"]\n",
    "    print(f\"Layer {layer}: AUROC={data['auroc']:.3f}, Acc={data['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Download probe files\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Downloading probe files...\")\n",
    "for layer in layers_to_test:\n",
    "    filename = f\"phi3_layer_{layer}_validation.pkl\"\n",
    "    try:\n",
    "        files.download(filename)\n",
    "        print(f\"✓ {filename}\")\n",
    "    except:\n",
    "        print(f\"✗ {filename} not found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}