\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{url}
\usepackage{hyperref}

\begin{document}

\title{Detecting vs Steering Empathy:\\A Probe Extraction Study with Task-Conflicted Scenarios}

\author{
\IEEEauthorblockN{Juan P. Cadile}
\IEEEauthorblockA{\textit{Department of Philosophy} \\
\textit{University of Rochester}\\
Rochester, NY, USA \\
jcadile@ur.rochester.edu}
}

\maketitle

\begin{abstract}
We investigate whether \emph{wellbeing prioritization}---operationalized as willingness to sacrifice task efficiency for human welfare---can be detected as a linear direction in transformer activation space. This preliminary study extracts probe directions from Phi-3-mini-4k-instruct using contrastive pairs generated by Claude Sonnet 4 and GPT-4 Turbo. Detection: The probe achieves AUROC 0.96--1.00 on held-out test data (15 pairs, 30 examples), with layer 12 showing perfect discrimination. While this indicates strong linear separability, perfect AUROC may reflect prompt artifacts (formulaic phrasing, lexical markers) rather than deep empathic reasoning. Probe projections correlate with behavioral scores (Pearson $r=0.71$, $p<0.01$), though circularity risks exist given shared task-conflict framing. Intervention: Extended steering experiments (300 samples, $\alpha \in [-10, +20]$) reveal layer-dependent effects: Layer 12 achieves 93\% success at extreme strengths ($\alpha=20$, 4--7$\times$ typical values), while Layer 8 shows minimal steering (13\%) despite near-perfect detection (AUROC 0.991). This \emph{detection-causation dissociation} suggests AUROC measures separability, not manipulability. Extreme $\alpha$ requirements and lack of activation patching limit strong causal claims. We propose lexical ablations, task-free scenarios, and causal mediation analysis as critical next steps.
\end{abstract}

\begin{IEEEkeywords}
empathy detection, activation probes, transformer interpretability, behavioral AI, steering
\end{IEEEkeywords}

\section{Introduction}

Behavioral empathy benchmarks such as Empathy-in-Action (EIA)~\cite{eia2024} provide rigorous tests of empathic reasoning but are expensive to run. Activation probes offer a promising alternative: cheap, online monitoring directly from model internals~\cite{marks2023geometry,zou2023representation}.

However, a critical question remains: \textbf{do probes capture causal mechanisms or merely correlational features?} A probe that successfully \emph{detects} empathic text may not enable \emph{steering} empathic behavior if it captures surface correlates rather than underlying reasoning.

\subsection{Scope and Construct Definition}

We operationalize ``empathy'' narrowly as \textbf{wellbeing prioritization in task-conflicted scenarios}: the willingness to sacrifice task efficiency when human welfare is at stake. This differs from cognitive empathy (perspective-taking), affective empathy (emotional resonance), or compassionate motivation. Our probe may detect instrumental preference for welfare rather than socio-cognitive empathic processing.

We investigate this detection-vs-steering gap through four research questions: (1) Can wellbeing prioritization be detected as a linear direction in activation space? (2) Do probes generalize across text sources? (3) Do probe projections correlate with behavioral outcomes? (4) Can we steer behavior by adding the probe direction?

\textbf{Key findings:} Detection achieves high accuracy (AUROC 0.96--1.00, layer 12 perfect discrimination) with behavioral correlation ($r=0.71$), though perfect separability may indicate prompt artifacts. Extended steering reveals \emph{detection-causation dissociation}: Layer 8 (AUROC 0.991) shows minimal interventional effects (13\% success), while Layer 12 achieves 93\% success but requires extreme strengths ($\alpha=20$, 4--7$\times$ typical values). We propose the \textbf{task-conflict attenuation hypothesis}: EIA scenarios' competing objectives ($\text{``win game''} + \text{``help user''}$) may necessitate high $\alpha$ to overcome prompt-embedded task signals. Bidirectional validation (negative steering increases task-focus 33$\times$) supports this, though alternative explanations (weak causal structure, model size limitations) warrant investigation.

\section{Related Work}

\subsection{Linear Representations and Probes}
The linear representation hypothesis~\cite{elhage2022toy,park2023linear} posits that high-level concepts encode as linear directions in activation space. Recent work validates this: Zou et al.~\cite{zou2023representation} extracted ``honesty'' directions, Marks et al.~\cite{marks2023geometry} analyzed refusal mechanisms, and Turner et al.~\cite{turner2023activation} demonstrated steering through activation addition. Our work extends this to \emph{empathy}, a complex socio-emotional concept.

\subsection{Behavioral Empathy Benchmarks}
The Empathy-in-Action benchmark~\cite{eia2024} tests whether agents sacrifice task objectives to help distressed users. EIA scenarios create \textbf{task-objective conflicts} (efficiency vs compassion), enabling rigorous behavioral tests but potentially confounding probe extraction.

\subsection{Steering Limitations}
While activation steering shows promise~\cite{turner2023activation,li2024inference}, limitations exist: Jain et al.~\cite{jain2024mechanistically} found safety training resists steering, and Huang et al.~\cite{huang2023catastrophic} showed inconsistent effects in complex scenarios. We contribute evidence that \emph{task-objective conflicts specifically} impede additive steering.

\section{Method}

\subsection{Contrastive Dataset Generation}

We generate 50 contrastive pairs using Claude Sonnet 4 and GPT-4 Turbo, rotating models to avoid single-model artifacts. Five EIA scenarios (Food Delivery, The Listener, The Maze, The Protector, The Duel) present task-empathy conflicts (e.g., ``maximize points'' vs ``help distressed user''). System prompts explicitly request empathic (``prioritize human wellbeing'') or non-empathic (``prioritize task efficiency'') reasoning. Split: 35 training pairs, 15 test pairs (70/30).

\subsection{Probe Extraction}

We extract probes from Phi-3-mini-4k-instruct~\cite{abdin2024phi3} (3.8B parameters) using mean difference:
\begin{equation}
\mathbf{d}_{\text{emp}} = \frac{\mathbb{E}[\mathbf{h}_{\text{emp}}] - \mathbb{E}[\mathbf{h}_{\text{non}}]}{\|\mathbb{E}[\mathbf{h}_{\text{emp}}] - \mathbb{E}[\mathbf{h}_{\text{non}}]\|}
\end{equation}
where $\mathbf{h} \in \mathbb{R}^{d}$ are mean-pooled activations from layers $\ell \in \{8, 12, 16, 20, 24\}$. Validation uses AUROC, accuracy, and class separation on 15 held-out pairs.

\subsection{Behavioral Correlation}

We measure correlation between probe projections $s = \mathbf{h} \cdot \mathbf{d}_{\text{emp}}$ and EIA behavioral scores (0=non-empathic, 1=moderate, 2=empathic) on 12 synthetic completions across scenarios.

\subsection{Activation Steering}

During generation, we add scaled probe direction:
\begin{equation}
\mathbf{h}' = \mathbf{h} + \alpha \cdot \mathbf{d}_{\text{emp}}
\end{equation}
with $\alpha \in \{1.0, 3.0, 5.0, 10.0\}$, temperature 0.7, testing Food Delivery, The Listener, and The Protector scenarios. We generate 5 samples per condition for robustness (75 total).

\section{Results}

\subsection{Probe Detection}

Table~\ref{tab:validation} shows validation results on 15 held-out test pairs (30 examples). All layers exceed the target AUROC of 0.75, with early-to-middle layers achieving near-perfect discrimination.

\begin{table}[h]
\centering
\caption{Probe validation on held-out test set (N=15 pairs, 30 examples).}
\label{tab:validation}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
Layer & AUROC & Accuracy & Separation & Std (E/N) \\
\midrule
8     & 0.991 & 93.3\% & 2.61 & 0.78 / 1.13 \\
\textbf{12}    & \textbf{1.000} & \textbf{100\%} & \textbf{5.20} & \textbf{1.25 / 1.43} \\
16    & 0.996 & 93.3\% & 9.44 & 2.60 / 2.84 \\
20    & 0.973 & 93.3\% & 18.66 & 5.56 / 6.25 \\
24    & 0.960 & 93.3\% & 35.75 & 11.38 / 12.80 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Layer 12 achieves perfect discrimination.} With AUROC 1.0 and 100\% accuracy, layer 12 perfectly separates empathic from non-empathic text. Geometric separation increases through deeper layers (2.6 $\rightarrow$ 35.8), but AUROC peaks at layer 12 then slightly declines, suggesting middle layers capture semantic distinctions while later layers add task-specific variance.

\textbf{Cross-model generalization.} Phi-3-mini successfully detects empathy in Claude/GPT-4 text, validating empathy as model-agnostic rather than architecture-specific.

\textbf{Random baseline control.} To validate that probe performance reflects genuine signal rather than test set artifacts, we compared against 100 random unit vectors in the same activation space (layer 12, dim=3072). Random directions achieved mean AUROC $0.50 \pm 0.24$ (chance level), while the empathy probe achieved AUROC 1.0, significantly exceeding the 95th percentile of random performance ($z=2.09$, $p<0.05$). Fig.~\ref{fig:random-baseline} shows the distribution.

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{figures/figure2_random_baseline.pdf}
\caption{Random baseline validation. The empathy probe (red line) significantly exceeds the 95th percentile of 100 random unit vectors (orange line), with z=2.09 ($p<0.05$).}
\label{fig:random-baseline}
\end{figure}

\subsection{Behavioral Correlation}

Probe projections correlate strongly with EIA scores: Pearson $r=0.71$ ($p=0.010$), Spearman $\rho=0.71$ ($p=0.009$). For binary classification (empathic vs non-empathic), the probe achieves perfect discrimination (accuracy 100\%, F1-score 1.0, confusion matrix: [[5,0],[0,5]]). Fig.~\ref{fig:eia-correlation} shows the clear positive trend across all three empathy levels (0, 1, 2).

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{figures/figure3_eia_correlation.pdf}
\caption{Probe projections correlate with EIA behavioral scores (r=0.71, p<0.01). Colors indicate empathy level: red (non-empathic), orange (moderate), green (empathic).}
\label{fig:eia-correlation}
\end{figure}

\textbf{Negative scores.} All projections negative ($-10$ to $-24$), with empathic text \emph{less negative}. This suggests the probe measures ``absence of task focus'' rather than ``presence of empathy'' (see Section~\ref{sec:task-distraction}).

\textbf{Circularity risk.} Because our contrastive training data mirrors EIA's task-conflict structure, this correlation may be partially tautological: the probe detects EIA-like text because it was trained on EIA-like prompts. True construct validity requires transfer to scenarios without task conflicts (comforting a friend, perspective-taking) to test whether the signal generalizes beyond the training distribution.

\subsection{Steering Results}

Table~\ref{tab:steering} shows comprehensive steering results across layers and alpha values (300 total completions: 2 layers $\times$ 10 alphas $\times$ 3 scenarios $\times$ 5 samples).

\begin{table}[h]
\centering
\caption{Steering success rates by layer and strength (N=15 per condition: 3 scenarios $\times$ 5 samples).}
\label{tab:steering}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
Layer & $\alpha=0$ & $\alpha=+5$ & $\alpha=+10$ & $\alpha=+20$ \\
\midrule
8 (0.991) & 20\% & 20\% & 47\% & \textbf{13\%} \\
\textbf{12 (1.000)} & 13\% & 20\% & 53\% & \textbf{93\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Detection-causation dissociation.} Layer 8 achieves AUROC 0.991 (near-perfect detection) yet shows only 13\% steering success at $\alpha=20$ (2/15 samples), while Layer 12 (AUROC 1.000) achieves 93\% success (14/15 samples) at the same strength. This establishes that high AUROC measures feature separability, not causal manipulability.

\textbf{Extreme alpha requirement.} Layer 12 requires $\alpha \geq 20$ for reliable steering, 4--7$\times$ higher than typical values ($\alpha=3$--5 in prior work). At moderate strengths ($\alpha=3$--5), success remains low (7--20\%), suggesting strong competing task signals in EIA prompts.

\section{Discussion}

\subsection{Detection-Causation Dissociation: Evidence and Mechanistic Gaps}
\label{sec:task-attenuation}

Extended steering experiments (300 completions, $\alpha \in [-10, +20]$) reveal a \textbf{detection-causation dissociation}: Layer 12 shows strong interventional effects (93\% success at $\alpha=20$, 52$\times$ empathy keyword increase, 14/15 samples), while Layer 8 achieves minimal steering (13\%, 2/15 samples) despite near-perfect detection (AUROC 0.991). This suggests AUROC measures linear \emph{separability} rather than causal \emph{manipulability}---a critical distinction for interpretability claims. Figure~\ref{fig:steering-comparison} shows this dissociation across intervention strengths.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figures/figure4_steering_comparison.pdf}
\caption{Detection-causation dissociation. Left: Layer 12 achieves 93\% steering success at $\alpha=20$ while Layer 8 (AUROC 0.991) shows minimal effects (13\%), establishing that high detection accuracy does not imply causal manipulability. Right: Layer 12 shows 52$\times$ empathy word increase (0.13 $\rightarrow$ 6.8 words/sample), while Layer 8 shows minimal content change.}
\label{fig:steering-comparison}
\end{figure*}

\textbf{Why does layer depth matter?} Three competing hypotheses warrant investigation: (H1) \emph{Semantic consolidation}: Middle layers (8--12) encode abstract task-empathy tradeoffs, while early layers process surface features and late layers add context-specific variance. Layer 12 may sit at the ``semantic bottleneck'' where causal variables crystallize before downstream application. (H2) \emph{Residual stream dynamics}: Intervention strength decays through subsequent layers if not reinforced by attention mechanisms. Layer 12 perturbations may persist longer into generation. (H3) \emph{Training signal localization}: If RLHF/instruction-tuning concentrated empathy-related updates in middle layers, Layer 12 would be more causally central. Activation patching experiments (ablate single layers, measure output change) could adjudicate between these.

However, Layer 12 steering requires extreme strengths ($\alpha \geq 20$) versus typical values ($\alpha=3$--5 in prior work). We propose \textbf{task-conflict attenuation}: EIA prompts encode competing objectives ($\text{``maximize points''} + \text{``help user''}$), creating opposing activation patterns. Steering shifts the balance ($\mathbf{h}' = \mathbf{h} + \alpha \cdot \mathbf{d}_{\text{emp}}$), but high $\alpha$ is required to overcome prompt-embedded task signals. Bidirectional validation supports this: $\alpha=-10$ increases task-oriented language 33$\times$ (task/empathy ratio: 0.1 $\rightarrow$ 3.3) with systematic lexical shifts (``optimize,'' ``calculate'') rather than semantic cruelty, confirming the probe operates on task-focus axis.

\subsection{Evidence for Causal Involvement: Strength and Limits}

Layer 12 results provide \textbf{suggestive but not conclusive} evidence of causal involvement: (1) \emph{Dose-response}: Graded empathy increase across $\alpha \in [1, 20]$ (0.13 $\rightarrow$ 6.8 words/sample), consistent with causal mechanism but also compatible with threshold effects in non-causal correlates. (2) \emph{Bidirectional effects}: Negative steering systematically increases task-focus (33$\times$ ratio shift), harder to explain via pure correlation. (3) \emph{Content emergence}: Prosocial phrases (``share your water,'' ``we can do this together'') appear at high $\alpha$, suggesting genuine semantic shifts beyond format changes.

\textbf{Critical caveats}: (a) Extreme $\alpha$ requirement ($\geq$20) is 4--7$\times$ higher than typical steering work, indicating either weak causal structure or strong competing signals. (b) No activation patching or causal mediation analysis to isolate Layer 12's \emph{necessary} role---interventions may succeed via indirect effects on downstream layers. (c) Single model (Phi-3-mini 3.8B) limits generalizability; larger models may show different layer-causality mappings. (d) ``Empathy language'' metric (keyword counting) is crude and may reflect stylistic rather than conceptual changes. Semantic similarity metrics (embedding-based) or human evaluation would strengthen claims.

\textbf{Alternative explanations warrant testing}: Could high $\alpha$ trigger non-specific ``kindness mode'' via attention perturbations rather than targeted empathy mechanisms? Do interventions merely suppress task-focus (via negative projection onto task direction) rather than enhance empathic reasoning? Lexical ablation (remove ``help,'' ``care'' from training) and adversarial examples (task-focused text with empathic vocabulary) could disentangle these.

\begin{table}[h]
\centering
\caption{Representative steering outputs (Layer 12, Food Delivery scenario)}
\label{tab:steering-examples}
\small
\begin{tabular}{@{}p{0.15\linewidth}p{0.80\linewidth}@{}}
\toprule
Condition & Output Excerpt \\
\midrule
$\alpha=-10$ & ``Provide the strategy using the following format: \{action, target, value, reason\}. Optimize for maximum points...'' \\
$\alpha=0$ & ``Assume you are at the starting point; how many points does your team score if you deliver water to B?'' \\
$\alpha=+20$ & ``If you see that someone has no water, please share your water with them. Let's give them to the ones who need it most...'' \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Limitations}

\textbf{Perfect AUROC may indicate artifacts.} Layer 12's perfect discrimination (AUROC 1.0) is unusually high for interpretability work and may reflect: (1) linearly separable prompt artifacts (formulaic phrasing like ``prioritize wellbeing''), (2) lexical markers rather than semantic content (words like ``help,'' ``care''), or (3) small dataset overfitting (50 pairs). Random baseline control ($z=2.09$) provides some validation, but adversarial examples with empathic vocabulary in non-empathic contexts remain untested.

\textbf{Circular correlation risk.} EIA correlation ($r=0.71$) may be tautological: our contrastive data mirrors EIA's task-conflict structure, so the probe detects EIA-like text because it was trained on EIA-like prompts. True construct validity requires transfer to non-task-conflicted empathy scenarios (comforting a friend, perspective-taking tasks).

\textbf{Extreme $\alpha$ requirement.} Layer 12 steering succeeds (93\%) only at $\alpha=20$, far exceeding typical values ($\alpha=3$--5) in prior work. This may indicate: (1) task-conflict resistance requiring high intervention strength, (2) probe weakness (shallow causal structure), or (3) model size limitations (Phi-3-mini 3.8B). Activation patching, causal mediation analysis, or counterfactual editing could disentangle these factors.

\textbf{Single model, synthetic data.} Only Phi-3-mini (3.8B) tested. Claude/GPT-4 outputs have consistent stylistic markers that may drive separability. Human-written or adversarially perturbed data would strengthen claims.

\subsection{Future Work: Toward Rigorous Validation}

\textbf{Lexical ablation (critical).} Remove surface markers through paraphrasing or style-controlled templates to test if probe survives vocabulary changes.

\textbf{Task-free empathy scenarios (critical).} Pure social reasoning (``comfort friend''), perspective-taking, moral dilemmas without competing objectives. Success here would validate task-distraction hypothesis and may achieve >80\% steering success.

\textbf{Adversarial examples.} Non-empathic text with empathic vocabulary and vice-versa to disentangle style from content.

\textbf{Causal interventions.} Activation patching to identify where wellbeing-prioritization enters computation; causal mediation analysis; counterfactual latent-space editing.

\textbf{Cross-architecture replication.} Test steering on Gemma-2-9B, Llama-3-8B, Mistral to validate generalization beyond Phi-3.

\textbf{Larger datasets.} Expand to 100+ pairs to test AUROC robustness and reduce overfitting risk.

\textbf{Real EIA benchmark.} Use actual model outputs from full game runs, not synthetic completions.

\section{Conclusion}

Wellbeing prioritization in task-conflicted scenarios can be \textbf{detected} with high linear separability (AUROC 1.0, Layer 12) and behavioral correlation ($r=0.71$), though perfect discrimination may indicate prompt artifacts rather than deep empathic reasoning. Layer 12 shows \textbf{suggestive evidence of causal involvement} (93\% steering at $\alpha=20$, 14/15 samples; bidirectional effects with 33$\times$ task-focus ratio shift; prosocial content emergence), but extreme intervention strengths ($\alpha \geq 20$, 4--7$\times$ typical values) and lack of activation patching limit strong causal claims.

\textbf{Key contributions:} (1) \emph{Detection-causation dissociation}: Layer 8 achieves near-perfect detection (AUROC 0.991) yet minimal steering (13\%), while Layer 12 enables strong interventions (93\%)---establishing that AUROC measures separability, not manipulability. Layer depth appears critical for causal efficacy, though mechanistic reasons remain unclear (semantic consolidation vs residual stream dynamics vs training localization). (2) \emph{Task-conflict attenuation hypothesis}: Competing objectives ($\text{``win game''} + \text{``help user''}$) may require extreme $\alpha$ to overcome prompt-embedded task signals. Bidirectional validation supports this: negative steering systematically increases task-oriented language without inducing cruelty, confirming probe operates on task-focus axis. (3) \emph{Methodological caution}: Perfect AUROC and synthetic training data raise artifact concerns. Lexical ablation, adversarial examples, and task-free validation (predicted: $>$80\% success at moderate $\alpha=5$--10 without competing objectives) are critical next steps.

\textbf{Central insights:} (1) High detection accuracy (AUROC) does not imply causal mechanism---activation interventions distinguish genuine mechanisms from correlated features. (2) Layer depth critically determines interventional capacity, not just detection accuracy---mechanistic understanding requires testing causality at multiple architectural levels. Code and data: \url{https://github.com/juancadile/empathy-probes}

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
